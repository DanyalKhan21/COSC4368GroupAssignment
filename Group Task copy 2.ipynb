{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8901b668-2e34-4407-8fb2-fca4c0a7d7c9",
   "metadata": {},
   "source": [
    "<h3>2-dimensional grid world for the PD-World as described</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a678d23-01af-4e70-8e22-8317b872c99d",
   "metadata": {},
   "source": [
    "Agents: agent 1 Black agent, agent 2  In red and 3 in blue  Dropoff: in green   Pick up: in yellow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59012e8-4710-4036-8aae-ec668848fd8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f379711-dd3f-4019-a25a-f576e736378e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from matplotlib.colors import ListedColormap\n",
    "# Define the grid size\n",
    "grid_size = 5\n",
    "custom_colors = ['white', 'black', 'red', 'blue', 'yellow','green']\n",
    "custom_cmap = ListedColormap(custom_colors)\n",
    "\n",
    "# Create a function to plot the grid\n",
    "def plot_grid(initial_state):\n",
    "    # Create a grid with all zeros\n",
    "    grid = np.zeros((grid_size, grid_size))\n",
    "\n",
    "    # Update the grid based on the initial state\n",
    "    for state in initial_state:\n",
    "        row, col, cell_type = state\n",
    "        if cell_type == 'Black':\n",
    "            grid[row-1][col-1] = 1  # Black agent represented by 1\n",
    "        elif cell_type == 'Red':\n",
    "            grid[row-1][col-1] = 2  # Red agent represented by 2\n",
    "        elif cell_type == 'Blue':\n",
    "            grid[row-1][col-1] = 3  # Blue agent represented by 3\n",
    "        elif cell_type == 'P':\n",
    "            grid[row-1][col-1] = 4  # Pickup cell represented by 4\n",
    "            \n",
    "        elif cell_type == 'D':\n",
    "            grid[row-1][col-1] = 5  # Dropoff cell represented by 5\n",
    "\n",
    "    # Set up the plot\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    sns.heatmap(grid, annot=True, cmap=custom_cmap, cbar=False, fmt='')\n",
    "\n",
    "    # Add labels to the axes\n",
    "    plt.xlabel('Columns')\n",
    "    plt.ylabel('Rows')\n",
    "\n",
    "    # Show the plot\n",
    "    plt.title('Grid World Representation')\n",
    "    plt.show()\n",
    "\n",
    "# Define the initial state\n",
    "initial_state = [\n",
    "    (1, 2, 'Black'),  # Black agent at row 1, column 3\n",
    "    (3, 3, 'Red'),    # Red agent at row 3, column 3\n",
    "    (5, 3, 'Blue'),   # Blue agent at row 3, column 5\n",
    "    (1, 5, 'P'),      # Pickup cell at row 5, column 3\n",
    "    (2, 4, 'P'),      # Pickup cell at row 2, column 4\n",
    "    (5, 2, 'P'),      # Pickup cell at row 1, column 5\n",
    "    (1, 1, 'D'),      # Dropoff cell at row 1, column 1\n",
    "    (3, 1, 'D'),      # Dropoff cell at row 3, column 1\n",
    "    (4, 5, 'D')       # Dropoff cell at row 4, column 5\n",
    "]\n",
    "\n",
    "# Plot the grid\n",
    "plot_grid(initial_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95909d1d-95fa-485e-b707-5ae1a3c9df02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7cd7fb2f-eac1-4144-aab1-d342ac923aa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tqdm\n",
      "  Obtaining dependency information for tqdm from https://files.pythonhosted.org/packages/2a/14/e75e52d521442e2fcc9f1df3c5e456aead034203d4797867980de558ab34/tqdm-4.66.2-py3-none-any.whl.metadata\n",
      "  Downloading tqdm-4.66.2-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m355.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.66.2-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m484.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tqdm\n",
      "Successfully installed tqdm-4.66.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "fd16e167-0e69-4c17-a2bd-f834fbcfa4cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5 0 1 0 4]\n",
      " [0 0 0 4 0]\n",
      " [5 0 2 0 0]\n",
      " [0 0 0 0 5]\n",
      " [0 4 3 0 0]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, color, initial_state):\n",
    "        self.color = color\n",
    "        self.state = list(initial_state)  # Convert tuple to list for mutability\n",
    "        self.blocks_carried = 0  # Number of blocks carried by the agent\n",
    "        self.q_table = {}  # Q-table for the agent\n",
    "        self.last_action = None  # Track last action taken by the agent\n",
    "\n",
    "    def take_action(self, epsilon):\n",
    "        positions = tuple(self.state[:6])  # Convert to tuple\n",
    "        block_status = tuple(self.state[6:9])  # Convert to tuple\n",
    "        block_counts = tuple(self.state[9:])  # Convert to tuple\n",
    "\n",
    "        # Define the set of possible actions\n",
    "        possible_actions = ['North', 'South', 'East', 'West', 'Pickup', 'Dropoff']\n",
    "\n",
    "        # Choose an action based on epsilon-greedy policy\n",
    "        if random.uniform(0, 1) < epsilon:  # Explore (random action)\n",
    "            action = random.choice(possible_actions)\n",
    "        else:  # Exploit (choose action with highest Q-value)\n",
    "            action_values = [self.q_table.get((tuple(self.state), a), 0) for a in possible_actions]\n",
    "            max_value = max(action_values)\n",
    "            best_actions = [a for a, v in zip(possible_actions, action_values) if v == max_value]\n",
    "            action = random.choice(best_actions)\n",
    "\n",
    "        # Apply the chosen action and get next state and reward\n",
    "        next_state, reward = self.apply_action(action, list(block_status), list(block_counts))\n",
    "\n",
    "        # Update Q-value based on Q-learning formula\n",
    "        old_q_value = self.q_table.get((tuple(self.state), self.last_action), 0)\n",
    "        max_next_q_value = max(self.q_table.get((next_state, a), 0) for a in self.get_valid_actions(next_state))\n",
    "        new_q_value = old_q_value + LEARNING_RATE * (reward + DISCOUNT_FACTOR * max_next_q_value - old_q_value)\n",
    "        self.q_table[(tuple(self.state), self.last_action)] = new_q_value\n",
    "\n",
    "        # Update agent's state and last action\n",
    "        self.state = list(next_state)  # Convert back to list\n",
    "        self.last_action = action\n",
    "\n",
    "        return next_state, reward\n",
    "\n",
    "    def apply_action(self, action, block_status, block_counts):\n",
    "        positions = list(self.state[:6])  # Convert to list\n",
    "\n",
    "        if action == 'North' and positions[1] > 1:\n",
    "            positions[1] -= 1  # Move north\n",
    "        elif action == 'South' and positions[1] < grid_size:\n",
    "            positions[1] += 1  # Move south\n",
    "        elif action == 'East' and positions[3] < grid_size:\n",
    "            positions[3] += 1  # Move east\n",
    "        elif action == 'West' and positions[3] > 1:\n",
    "            positions[3] -= 1  # Move west\n",
    "        elif action == 'Pickup':\n",
    "            # Check if the agent is in a pickup cell and not already carrying a block\n",
    "            for pickup_loc in pickup_locations:\n",
    "                if positions[:2] == list(pickup_loc[:2]) and block_status[pickup_loc[2] - 1] == 1 and self.blocks_carried == 0:\n",
    "                    block_counts[pickup_loc[2] - 1] -= 1  # Reduce the block count at the pickup cell\n",
    "                    self.blocks_carried += 1  # Increase the agent's block count\n",
    "                    return tuple(positions + block_status + block_counts), REWARD_PICKUP\n",
    "        elif action == 'Dropoff':\n",
    "            # Check if the agent is in a dropoff cell and carrying a block\n",
    "            for dropoff_loc in dropoff_locations:\n",
    "                if positions[:2] == list(dropoff_loc[:2]) and block_status[dropoff_loc[2] - 1] == 0 and self.blocks_carried > 0:\n",
    "                    block_counts[dropoff_loc[2] - 1] += 1  # Increase the block count at the dropoff cell\n",
    "                    self.blocks_carried -= 1  # Decrease the agent's block count\n",
    "                    return tuple(positions + block_status + block_counts), REWARD_DROPOFF\n",
    "\n",
    "        # For all other actions or invalid actions, return the current state and a move reward\n",
    "        return tuple(positions + block_status + block_counts), REWARD_MOVE\n",
    "\n",
    "    def get_valid_actions(self, state):\n",
    "        # Implement logic to get valid actions based on the current state\n",
    "        valid_actions = ['North', 'South', 'East', 'West']  # By default, the agent can move in all directions\n",
    "\n",
    "        # Check if the agent is in a pickup cell and not already carrying a block\n",
    "        for pickup_loc in pickup_locations:\n",
    "            if state[:2] == list(pickup_loc[:2]) and self.blocks_carried == 0:\n",
    "                valid_actions.append('Pickup')  # Add pickup action to valid actions\n",
    "\n",
    "        # Check if the agent is in a dropoff cell and carrying a block\n",
    "        for dropoff_loc in dropoff_locations:\n",
    "            if state[:2] == list(dropoff_loc[:2]) and self.blocks_carried > 0:\n",
    "                valid_actions.append('Dropoff')  # Add dropoff action to valid actions\n",
    "\n",
    "        return valid_actions  # Return the list of valid actions\n",
    "\n",
    "# Define constants\n",
    "grid_size = 5\n",
    "REWARD_PICKUP = 13\n",
    "REWARD_DROPOFF = 13\n",
    "REWARD_MOVE = -1\n",
    "NUM_EPISODES = 4\n",
    "EPSILON_DECAY = 0.99\n",
    "LEARNING_RATE = 0.1\n",
    "DISCOUNT_FACTOR = 0.9\n",
    "\n",
    "\n",
    "initial_environment = np.array([\n",
    "    [5, 0, 1, 0, 4],\n",
    "    [0, 0, 0, 4, 0],\n",
    "    [5, 0, 2, 0, 0],\n",
    "    [0, 0, 0, 0, 5],\n",
    "    [0, 4, 3, 0, 0]\n",
    "])\n",
    "\n",
    "print(initial_environment)\n",
    "states = [(i, j, i_, j_, i__, j_) for i in range(1, 6) for j in range(1, 6)\n",
    "          for i_ in range(1, 6) for j_ in range(1, 6) for i__ in range(1, 6)]\n",
    "\n",
    "actions = ['North', 'South', 'East', 'West', 'Pickup', 'Dropoff']\n",
    "\n",
    "# Define agents\n",
    "red_agent = Agent('Red', (3, 3, 3, 5, 1, 3, 0, 0, 0, 0, 0, 0, 5, 5, 5))\n",
    "blue_agent = Agent('Blue', (5, 3, 3, 3, 1, 3, 0, 0, 0, 0, 0, 0, 5, 5, 5))\n",
    "black_agent = Agent('Black', (1, 3, 3, 3, 1, 3, 0, 0, 0, 0, 0, 0, 5, 5, 5))\n",
    "\n",
    "# Define pickup and dropoff locations\n",
    "pickup_locations = [\n",
    "    (1, 5, 0),  \n",
    "    (2, 4, 1),      # Pickup cell at row 2, column 4\n",
    "    (5, 2, 2)       # Pickup cell at row 5, column 2\n",
    "]\n",
    "\n",
    "dropoff_locations = [\n",
    "    (1, 1, 0),      # Dropoff cell at row 1, column 1\n",
    "    (3, 1, 1),      # Dropoff cell at row 3, column 1\n",
    "    (4, 5, 2)       # Dropoff cell at row 4, column 5\n",
    "]\n",
    "\n",
    "# Function to check terminal state\n",
    "def is_terminal_state(environment):\n",
    "    # Iterate through each drop-off location\n",
    "    for dropoff_loc in dropoff_locations:\n",
    "        row, col, _ = dropoff_loc  # Extract row and column\n",
    "        if environment[row - 1, col - 1] != 5:  # Check if the cell doesn't contain 5 blocks\n",
    "            return False  # Not a terminal state\n",
    "    return True  # All drop-off cells contain 5 blocks, so it's a terminal state\n",
    "\n",
    "\n",
    "def update_q_values(agent, next_state, reward, learning_rate, discount_factor):\n",
    "    old_q_value = agent.q_table.get((tuple(agent.state), agent.last_action), 0)\n",
    "    max_next_q_value = max(agent.q_table.get((next_state, a), 0) for a in agent.get_valid_actions(next_state))\n",
    "    new_q_value = old_q_value + learning_rate * (reward + discount_factor * max_next_q_value - old_q_value)\n",
    "    agent.q_table[(tuple(agent.state), agent.last_action)] = new_q_value\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e719ce-affe-4593-9bf9-96a28c76aee0",
   "metadata": {},
   "source": [
    "<h3>dd</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "a7e52a4f-6c53-4c23-a2c7-532a926290b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training Progress: 100%|████████████████████████| 4/4 [00:00<00:00, 4065.23it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards per Episode: [-3, -3, -3, -3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def train_agents(num_episodes, epsilon_decay, learning_rate, discount_factor, epsilon):\n",
    "    # Initialize Q-table for each agent\n",
    "    red_agent.q_table = {}\n",
    "    blue_agent.q_table = {}\n",
    "    black_agent.q_table = {}\n",
    "\n",
    "    rewards = []\n",
    "\n",
    "    for episode in tqdm(range(num_episodes), desc='Training Progress'):\n",
    "        # Reset environment and agents to initial state\n",
    "        environment = np.copy(initial_environment)\n",
    "        red_agent.state = (3, 3, 3, 5, 1, 3, 0, 0, 0, 0, 0, 0, 5, 5, 5)\n",
    "        blue_agent.state = (5, 3, 3, 3, 1, 3, 0, 0, 0, 0, 0, 0, 5, 5, 5)\n",
    "        black_agent.state = (1, 3, 3, 3, 1, 3, 0, 0, 0, 0, 0, 0, 5, 5, 5)\n",
    "\n",
    "        total_reward = 0\n",
    "\n",
    "        while True:\n",
    "            # Agents take actions\n",
    "            red_next_state, red_reward = red_agent.take_action(epsilon)\n",
    "            blue_next_state, blue_reward = blue_agent.take_action(epsilon)\n",
    "            black_next_state, black_reward = black_agent.take_action(epsilon)\n",
    "\n",
    "            # Update Q-values for each agent\n",
    "            update_q_values(red_agent, red_next_state, red_reward, learning_rate, discount_factor)\n",
    "            update_q_values(blue_agent, blue_next_state, blue_reward, learning_rate, discount_factor)\n",
    "            update_q_values(black_agent, black_next_state, black_reward, learning_rate, discount_factor)\n",
    "\n",
    "            total_reward += red_reward + blue_reward + black_reward\n",
    "\n",
    "            # Check for terminal state\n",
    "            if is_terminal_state(environment):\n",
    "                break\n",
    "\n",
    "        # Decay epsilon after each episode\n",
    "        epsilon *= epsilon_decay\n",
    "\n",
    "        rewards.append(total_reward)\n",
    "\n",
    "    # Return rewards and trained agents\n",
    "    return rewards, [red_agent, blue_agent, black_agent]\n",
    "\n",
    "# Train agents\n",
    "rewards, trained_agents = train_agents(NUM_EPISODES, EPSILON_DECAY, LEARNING_RATE, DISCOUNT_FACTOR, 1.0)\n",
    "\n",
    "# Print rewards per episode\n",
    "print(\"Rewards per Episode:\", rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "08b66d91-ec2c-4c22-97db-ba4af7b3b345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Red Agent Q-Table:\n",
      "                                                    Q-Value\n",
      "((3, 3, 3, 5, 1, 3, 0, 0, 0, 0, 0, 0, 5, 5, 5),...   -0.100\n",
      "((3, 3, 3, 4, 1, 3, 0, 0, 0, 0, 0, 0, 5, 5, 5),...   -0.100\n",
      "((3, 3, 3, 5, 1, 3, 0, 0, 0, 0, 0, 0, 5, 5, 5),...   -0.100\n",
      "((3, 3, 3, 5, 1, 3, 0, 0, 0, 0, 0, 0, 5, 5, 5),...   -0.190\n",
      "((3, 3, 3, 5, 1, 3, 0, 0, 0, 0, 0, 0, 5, 5, 5),...   -0.271\n",
      "\n",
      "Blue Agent Q-Table:\n",
      "                                                    Q-Value\n",
      "((5, 3, 3, 3, 1, 3, 0, 0, 0, 0, 0, 0, 5, 5, 5),...    -0.10\n",
      "((5, 4, 3, 3, 1, 3, 0, 0, 0, 0, 0, 0, 5, 5, 5),...    -0.10\n",
      "((5, 3, 3, 3, 1, 3, 0, 0, 0, 0, 0, 0, 5, 5, 5),...    -0.10\n",
      "((5, 3, 3, 3, 1, 3, 0, 0, 0, 0, 0, 0, 5, 5, 5),...    -0.19\n",
      "((5, 3, 3, 3, 1, 3, 0, 0, 0, 0, 0, 0, 5, 5, 5),...    -0.19\n",
      "((5, 3, 3, 4, 1, 3, 0, 0, 0, 0, 0, 0, 5, 5, 5),...    -0.10\n",
      "\n",
      "Black Agent Q-Table:\n",
      "                                                    Q-Value\n",
      "((1, 3, 3, 3, 1, 3, 0, 0, 0, 0, 0, 0, 5, 5, 5),...    -0.19\n",
      "((1, 3, 3, 2, 1, 3, 0, 0, 0, 0, 0, 0, 5, 5, 5),...    -0.10\n",
      "((1, 3, 3, 3, 1, 3, 0, 0, 0, 0, 0, 0, 5, 5, 5),...    -0.10\n",
      "((1, 3, 3, 4, 1, 3, 0, 0, 0, 0, 0, 0, 5, 5, 5),...    -0.10\n",
      "((1, 3, 3, 3, 1, 3, 0, 0, 0, 0, 0, 0, 5, 5, 5),...    -0.10\n",
      "((1, 2, 3, 3, 1, 3, 0, 0, 0, 0, 0, 0, 5, 5, 5),...    -0.10\n",
      "((1, 3, 3, 3, 1, 3, 0, 0, 0, 0, 0, 0, 5, 5, 5),...    -0.10\n"
     ]
    }
   ],
   "source": [
    "def visualize_q_table(agent):\n",
    "    q_table_df = pd.DataFrame.from_dict(agent.q_table, orient='index', columns=['Q-Value'])\n",
    "    return q_table_df\n",
    "red_q_table = visualize_q_table(trained_agents[0])\n",
    "print(\"Red Agent Q-Table:\")\n",
    "print(red_q_table)\n",
    "\n",
    "blue_q_table = visualize_q_table(trained_agents[1])\n",
    "print(\"\\nBlue Agent Q-Table:\")\n",
    "print(blue_q_table)\n",
    "\n",
    "black_q_table = visualize_q_table(trained_agents[2])\n",
    "print(\"\\nBlack Agent Q-Table:\")\n",
    "print(black_q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304ce5ac-6940-442d-b697-fc26f2555f4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
