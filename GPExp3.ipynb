{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4696d4f6-0161-4d28-bd5b-b33a34b990e9",
   "metadata": {},
   "source": [
    "<h3> Base Code , Polices , Training  </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b24040-96ce-4a02-ac79-38012710a8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from itertools import combinations\n",
    "\n",
    "GRID_SIZE = 5\n",
    "PICKUP_LOCATIONS = {(0, 4): 5, (1, 3): 5, (4, 1): 5}  # Pickup locations with their capacities\n",
    "DROPOFF_LOCATIONS = {(0, 0): 0, (2, 0): 0, (3, 4): 0}  # Dropoff locations with their remaining capacities\n",
    "ACTIONS = ['N', 'E', 'S', 'W', 'P', 'D']  # Actions: North, East, South, West, Pickup, Dropoff\n",
    "ACTION_REWARDS = {'P': 13, 'D': 13, 'N': -1, 'E': -1, 'S': -1, 'W': -1}  # Reward values for actions\n",
    "random.seed(4)\n",
    "np.random.seed(4)\n",
    "\n",
    "# Hyperparameters\n",
    "LEARNING_RATE = 0.1\n",
    "DISCOUNT_FACTOR = 0.95\n",
    "EPSILON = 0.99\n",
    "EPISODES = 1000\n",
    "\n",
    "# Initial positions for each agent\n",
    "initial_positions = {'red': (2, 2), 'blue': (4, 2), 'black': (0, 2)}  # Adjusted to 0-based indexing\n",
    "\n",
    "# Agent class definition\n",
    "class Agent:\n",
    "    def __init__(self, start_position, name):\n",
    "        self.position = start_position\n",
    "        self.name = name\n",
    "        self.carrying = False\n",
    "        self.q_table = np.zeros((GRID_SIZE, GRID_SIZE, len(ACTIONS)))\n",
    "\n",
    "    def get_valid_actions(self):\n",
    "        x, y = self.position\n",
    "        valid_actions = ACTIONS.copy()\n",
    "\n",
    "        # Check if agent is at a border\n",
    "        if x == 0: valid_actions.remove('N')\n",
    "        if x == GRID_SIZE - 1: valid_actions.remove('S')\n",
    "        if y == 0: valid_actions.remove('W')\n",
    "        if y == GRID_SIZE - 1: valid_actions.remove('E')\n",
    "\n",
    "        # Check for pickup/dropoff actions\n",
    "        if (x, y) in PICKUP_LOCATIONS and (PICKUP_LOCATIONS[(x, y)] <= 0 or self.carrying):\n",
    "            valid_actions.remove('P')\n",
    "        if (x, y) in DROPOFF_LOCATIONS and (DROPOFF_LOCATIONS[(x, y)] >= 5 or not self.carrying):\n",
    "            valid_actions.remove('D')\n",
    "        return valid_actions\n",
    "\n",
    "    \n",
    "    def select_action(self, valid_actions, policy='PRandom'):        \n",
    "        if not valid_actions:\n",
    "            return None  # No valid actions available\n",
    "        if policy == 'PRandom' or (policy == 'PExploit' and random.random() < EPSILON):\n",
    "            return random.choice(valid_actions)\n",
    "        else:\n",
    "            q_values = [self.q_table[x, y, ACTIONS.index(a)] for a in valid_actions]\n",
    "            max_q_value = max(q_values)\n",
    "            max_actions = [a for a, q in zip(valid_actions, q_values) if q == max_q_value]\n",
    "            return random.choice(max_actions)\n",
    "\n",
    "    def perform_action(self, action):  \n",
    "        x, y = self.position\n",
    "\n",
    "        # Update position based on action\n",
    "        if action == 'N': x = max(0, x - 1)\n",
    "        elif action == 'S': x = min(GRID_SIZE - 1, x + 1)\n",
    "        elif action == 'E': y = min(GRID_SIZE - 1, y + 1)\n",
    "        elif action == 'W': y = max(0, y - 1)\n",
    "\n",
    "        # Handle pickup and dropoff actions\n",
    "        if action == 'P' and (x, y) in PICKUP_LOCATIONS and PICKUP_LOCATIONS[(x, y)] > 0:\n",
    "            self.carrying = True\n",
    "            PICKUP_LOCATIONS[(x, y)] -= 1\n",
    "        elif action == 'D' and (x, y) in DROPOFF_LOCATIONS and DROPOFF_LOCATIONS[(x, y)] < 5:\n",
    "            self.carrying = False\n",
    "            DROPOFF_LOCATIONS[(x, y)] += 1\n",
    "\n",
    "        self.position = (x, y)\n",
    "        return self.position\n",
    "\n",
    "    def update_q_table(self, action, reward, next_state, next_valid_actions):\n",
    "        if action is None:\n",
    "            return  # Skip Q-table update if no action was taken\n",
    "        old_x, old_y = self.position\n",
    "        new_x, new_y = next_state\n",
    "        action_index = ACTIONS.index(action)\n",
    "        future_rewards = [self.q_table[new_x, new_y, ACTIONS.index(a)] for a in next_valid_actions]\n",
    "        self.q_table[old_x, old_y, action_index] = (1 - LEARNING_RATE) * self.q_table[old_x, old_y, action_index] + \\\n",
    "            LEARNING_RATE * (reward + DISCOUNT_FACTOR * np.max(future_rewards))\n",
    "\n",
    "    def is_terminal_state(self):\n",
    "        return all(blocks == 5 for blocks in DROPOFF_LOCATIONS.values())\n",
    "\n",
    "    def reset(self):\n",
    "        initial_positions = {'red': (2, 2), 'blue': (4, 2), 'black': (0, 2)}\n",
    "        self.position = initial_positions[self.name]\n",
    "        self.carrying = False\n",
    "        for loc in PICKUP_LOCATIONS:\n",
    "            PICKUP_LOCATIONS[loc] = 5\n",
    "        for loc in DROPOFF_LOCATIONS:\n",
    "            DROPOFF_LOCATIONS[loc] = 0\n",
    "\n",
    "# Utility functions\n",
    "def manhattan_distance(pos1, pos2):\n",
    "    return abs(pos1[0] - pos2[0]) + abs(pos1[1] - pos2[1])\n",
    "\n",
    "def average_manhattan_distance(agents):\n",
    "    distances = [manhattan_distance(agent1.position, agent2.position) \n",
    "                 for agent1, agent2 in combinations(agents, 2)]\n",
    "    return np.mean(distances)\n",
    "\n",
    "def get_shortest_path_length(start, end):\n",
    "    return abs(start[0] - end[0]) + abs(start[1] - end[1])\n",
    "\n",
    "def simulate_agent_path(agent, start, end):\n",
    "    agent.reset()\n",
    "    agent.position = start\n",
    "    path = [start]\n",
    "    while agent.position != end:\n",
    "        valid_actions = agent.get_valid_actions()\n",
    "        action = agent.select_action(valid_actions)\n",
    "        agent.perform_action(action)\n",
    "        path.append(agent.position)\n",
    "    return path\n",
    "\n",
    "# Simulation\n",
    "agents = [Agent(initial_positions['red'], 'red'), \n",
    "          Agent(initial_positions['blue'], 'blue'), \n",
    "          Agent(initial_positions['black'], 'black')]\n",
    "\n",
    "average_distances = []\n",
    "\n",
    "for episode in range(EPISODES):\n",
    "    episode_distances = []\n",
    "    for agent in agents:\n",
    "        valid_actions = agent.get_valid_actions()\n",
    "        action = agent.select_action(valid_actions, policy='PRandom')  \n",
    "        if action:\n",
    "            new_position = agent.perform_action(action)\n",
    "            reward = ACTION_REWARDS.get(action, 0)\n",
    "            next_valid_actions = agent.get_valid_actions()\n",
    "            agent.update_q_table(action, reward, new_position, next_valid_actions)\n",
    "        episode_distances.append(average_manhattan_distance(agents))\n",
    "    average_distances.append(np.mean(episode_distances))\n",
    "    if agent.is_terminal_state():\n",
    "        for a in agents:\n",
    "            a.reset()\n",
    "        break\n",
    "\n",
    "# Output analysis\n",
    "print(f\"Average Manhattan distances over episodes:\")\n",
    "for i, avg_dist in enumerate(average_distances, 1):\n",
    "    print(f\"Episode {i}: {avg_dist:.2f}\")\n",
    "\n",
    "# Output Q-tables for analysis\n",
    "for agent in agents:\n",
    "    print(f\"{agent.name}'s Q-table:\")\n",
    "    print(\"State  |   S       W       N       E       P       D\")\n",
    "    print(\"---------------------------------------------------\")\n",
    "    for x in range(GRID_SIZE):\n",
    "        for y in range(GRID_SIZE):\n",
    "            q_values = agent.q_table[x, y]\n",
    "            print(f\"({x+1},{y+1})  |  \" + \"  \".join(f\"{q_value:6.2f}\" for q_value in q_values))\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Simulate a path from a pickup to a dropoff location\n",
    "agent = Agent(initial_positions['red'], 'red')  # You can choose any agent here\n",
    "pickup_location = next(iter(PICKUP_LOCATIONS))\n",
    "dropoff_location = next(iter(DROPOFF_LOCATIONS))\n",
    "path = simulate_agent_path(agent, pickup_location, dropoff_location)\n",
    "\n",
    "# Calculate the shortest path length\n",
    "shortest_path_len = get_shortest_path_length(pickup_location, dropoff_location)\n",
    "\n",
    "print(f\"Simulated path: {path}\")\n",
    "print(f\"Simulated path length: {len(path) - 1}\")\n",
    "print(f\"Shortest path length: {shortest_path_len}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3fb4ab8-d005-4e81-a78f-97d56e475bff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dc1a6160-af11-4c23-801a-27e2153e0711",
   "metadata": {},
   "source": [
    "a.\tAgent coordination: Do the three agents get in their ways blocking each other or do they do a good job in dividing the transportation task intelligently among one another. Agent coordination could, for example, be measured by computing the average Manhattan distance between the three agents during the run of a specific experiment. You can show the progress of average distance for different iterations. b.\tPaths learned: Does the particular approach do a good job in learning paths between block sources and block destinations; is the learnt path the shortest path or close to the shortest path between the source and the destination.  c.\tCost Efficient Learning: Does the particular approach do a good job learning efficient paths or avoiding risky areas. How the risky areas are traversed by the agents as the training goes on and is there any avoidance procedure followed by the trained agents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db812d76-a9a0-42c7-87dc-3f098ebbe8fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcce1773-434c-4d70-b701-c4617c5397df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28c6866-bec8-413c-a4b4-3fe4bba4f3db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2e90e3c4-a150-474f-9375-80166e20576f",
   "metadata": {},
   "source": [
    "Extra Credit On The Compare   For the task 2 and 3 below\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db05ff9-e5cb-41c2-b59e-e1354dbb71ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c0c745-2176-49b0-a4f0-7ac9aedf34e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7a38d6b5-20fb-4211-83f8-ffdc621345a9",
   "metadata": {},
   "source": [
    "<h3> Experiment 1    Alpha=0.3 Y=0.5   Steps=9000   , PRandom=500 </h3>    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4faf64-b1e3-447d-a54d-b51a26bf6e39",
   "metadata": {},
   "source": [
    "<h3> EXP1 A.PRandom=8500</h3>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "882c8943-c490-49a9-953f-6a06a6e1c3d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "red's Q-table after Experiment 1a:\n",
      "State  |   N       E       S       W       P       D\n",
      "---------------------------------------------------\n",
      "(1,1)  |   12.00    0.00    0.00   12.00   26.00   26.00\n",
      "(1,2)  |   12.00   12.00    0.00   12.00   26.00   26.00\n",
      "(1,3)  |   12.00   12.00    0.00   12.00   26.00   26.00\n",
      "(1,4)  |   12.00   12.00    0.00   12.00   26.00   26.00\n",
      "(1,5)  |   12.00   12.00    0.00    0.00   26.00   26.00\n",
      "(2,1)  |   12.00    0.00   12.00   12.00   26.00   26.00\n",
      "(2,2)  |   12.00   12.00   12.00   12.00   26.00   26.00\n",
      "(2,3)  |   12.00   12.00   12.00   12.00   26.00   26.00\n",
      "(2,4)  |   12.00   12.00   12.00   12.00   26.00   26.00\n",
      "(2,5)  |   12.00   12.00   12.00    0.00   26.00   26.00\n",
      "(3,1)  |   12.00    0.00   12.00   12.00   26.00   26.00\n",
      "(3,2)  |   12.00   12.00   12.00   12.00   26.00   26.00\n",
      "(3,3)  |   12.00   12.00   12.00   12.00   26.00   26.00\n",
      "(3,4)  |   12.00   12.00   12.00   12.00   26.00   26.00\n",
      "(3,5)  |   12.00   12.00   12.00    0.00   26.00   26.00\n",
      "(4,1)  |   12.00    0.00   12.00   12.00   26.00   26.00\n",
      "(4,2)  |   12.00   12.00   12.00   12.00   26.00   26.00\n",
      "(4,3)  |   12.00   12.00   12.00   12.00   26.00   26.00\n",
      "(4,4)  |   12.00   12.00   12.00   12.00   26.00   26.00\n",
      "(4,5)  |   12.00   12.00   12.00    0.00   26.00   26.00\n",
      "(5,1)  |    0.00    0.00   12.00   12.00   26.00   26.00\n",
      "(5,2)  |    0.00   12.00   12.00   12.00   26.00   26.00\n",
      "(5,3)  |    0.00   12.00   12.00   12.00   26.00   26.00\n",
      "(5,4)  |    0.00   12.00   12.00   12.00   26.00   26.00\n",
      "(5,5)  |    0.00   12.00   12.00    0.00   26.00   26.00\n",
      "\n",
      "\n",
      "blue's Q-table after Experiment 1a:\n",
      "State  |   N       E       S       W       P       D\n",
      "---------------------------------------------------\n",
      "(1,1)  |   12.00    0.00    0.00   12.00   26.00   26.00\n",
      "(1,2)  |   12.00   12.00    0.00   12.00   26.00   26.00\n",
      "(1,3)  |   12.00   12.00    0.00   12.00   26.00   26.00\n",
      "(1,4)  |   12.00   12.00    0.00   12.00   26.00   26.00\n",
      "(1,5)  |   12.00   12.00    0.00    0.00   26.00   26.00\n",
      "(2,1)  |   12.00    0.00   12.00   12.00   26.00   26.00\n",
      "(2,2)  |   12.00   12.00   12.00   12.00   26.00   26.00\n",
      "(2,3)  |   12.00   12.00   12.00   12.00   26.00   26.00\n",
      "(2,4)  |   12.00   12.00   12.00   12.00   26.00   26.00\n",
      "(2,5)  |   12.00   12.00   12.00    0.00   26.00   26.00\n",
      "(3,1)  |   12.00    0.00   12.00   12.00   26.00   26.00\n",
      "(3,2)  |   12.00   12.00   12.00   12.00   26.00   26.00\n",
      "(3,3)  |   12.00   12.00   12.00   12.00   26.00   26.00\n",
      "(3,4)  |   12.00   12.00   12.00   12.00   26.00   26.00\n",
      "(3,5)  |   12.00   12.00   12.00    0.00   26.00   26.00\n",
      "(4,1)  |   12.00    0.00   12.00   12.00   26.00   26.00\n",
      "(4,2)  |   12.00   12.00   12.00   12.00   26.00   26.00\n",
      "(4,3)  |   12.00   12.00   12.00   12.00   26.00   26.00\n",
      "(4,4)  |   12.00   12.00   12.00   12.00   26.00   26.00\n",
      "(4,5)  |   12.00   12.00   12.00    0.00   26.00   26.00\n",
      "(5,1)  |    0.00    0.00   12.00   12.00   26.00   26.00\n",
      "(5,2)  |    0.00   12.00   12.00   12.00   26.00   26.00\n",
      "(5,3)  |    0.00   12.00   12.00   12.00   26.00   26.00\n",
      "(5,4)  |    0.00   12.00   12.00   12.00   26.00   26.00\n",
      "(5,5)  |    0.00   12.00   12.00    0.00   26.00   26.00\n",
      "\n",
      "\n",
      "black's Q-table after Experiment 1a:\n",
      "State  |   N       E       S       W       P       D\n",
      "---------------------------------------------------\n",
      "(1,1)  |   12.00    0.00    0.00   12.00   26.00   26.00\n",
      "(1,2)  |   12.00   12.00    0.00   12.00   26.00   26.00\n",
      "(1,3)  |   12.00   12.00    0.00   12.00   26.00   26.00\n",
      "(1,4)  |   12.00   12.00    0.00   12.00   26.00   26.00\n",
      "(1,5)  |   12.00   12.00    0.00    0.00   26.00   26.00\n",
      "(2,1)  |   12.00    0.00   12.00   12.00   26.00   26.00\n",
      "(2,2)  |   12.00   12.00   12.00   12.00   26.00   26.00\n",
      "(2,3)  |   12.00   12.00   12.00   12.00   26.00   26.00\n",
      "(2,4)  |   12.00   12.00   12.00   12.00   26.00   26.00\n",
      "(2,5)  |   12.00   12.00   12.00    0.00   26.00   26.00\n",
      "(3,1)  |   12.00    0.00   12.00   12.00   26.00   26.00\n",
      "(3,2)  |   12.00   12.00   12.00   12.00   26.00   26.00\n",
      "(3,3)  |   12.00   12.00   12.00   12.00   26.00   26.00\n",
      "(3,4)  |   12.00   12.00   12.00   12.00   26.00   26.00\n",
      "(3,5)  |   12.00   12.00   12.00    0.00   26.00   26.00\n",
      "(4,1)  |   12.00    0.00   12.00   12.00   26.00   26.00\n",
      "(4,2)  |   12.00   12.00   12.00   12.00   26.00   26.00\n",
      "(4,3)  |   12.00   12.00   12.00   12.00   26.00   26.00\n",
      "(4,4)  |   12.00   12.00   12.00   12.00   26.00   26.00\n",
      "(4,5)  |   12.00   12.00   12.00    0.00   26.00   26.00\n",
      "(5,1)  |    0.00    0.00   12.00   12.00   26.00   26.00\n",
      "(5,2)  |    0.00   12.00   12.00   12.00   26.00   26.00\n",
      "(5,3)  |    0.00   12.00   12.00   12.00   26.00   26.00\n",
      "(5,4)  |    0.00   12.00   12.00   12.00   26.00   26.00\n",
      "(5,5)  |    0.00   12.00   12.00    0.00   26.00   26.00\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Define the environment setup\n",
    "GRID_SIZE = 5\n",
    "PICKUP_LOCATIONS = {(0, 4): 5, (1, 3): 5, (4, 1): 5}\n",
    "DROPOFF_LOCATIONS = {(0, 0): 0, (2, 0): 0, (3, 4): 0}\n",
    "ACTIONS = ['N', 'E', 'S', 'W', 'P', 'D']\n",
    "ACTION_REWARDS = {'P': 13, 'D': 13, 'N': -1, 'E': -1, 'S': -1, 'W': -1}\n",
    "\n",
    "# Hyperparameters\n",
    "LEARNING_RATE = 0.3\n",
    "DISCOUNT_FACTOR = 0.5\n",
    "STEPS_PRANDOM_INITIAL = 500  # Initial random steps\n",
    "STEPS_PRANDOM_EXTENDED = 8500  # Extended random steps\n",
    "\n",
    "# Seed for reproducibility\n",
    "random.seed(6)\n",
    "np.random.seed(6)\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, start_position, name):\n",
    "        self.position = start_position\n",
    "        self.name = name\n",
    "        self.carrying = False\n",
    "        self.q_table = np.zeros((GRID_SIZE, GRID_SIZE, len(ACTIONS)))\n",
    "\n",
    "    def get_valid_actions(self):\n",
    "        x, y = self.position\n",
    "        valid_actions = ACTIONS.copy()\n",
    "        if x == 0: valid_actions.remove('N')\n",
    "        if x == GRID_SIZE - 1: valid_actions.remove('S')\n",
    "        if y == 0: valid_actions.remove('W')\n",
    "        if y == GRID_SIZE - 1: valid_actions.remove('E')\n",
    "        if self.carrying and (x, y) in DROPOFF_LOCATIONS:\n",
    "            valid_actions.append('D')\n",
    "        elif not self.carrying and (x, y) in PICKUP_LOCATIONS and PICKUP_LOCATIONS[(x, y)] > 0:\n",
    "            valid_actions.append('P')\n",
    "        return valid_actions\n",
    "\n",
    "    def select_action(self, valid_actions, policy):\n",
    "        if not valid_actions:\n",
    "            return None\n",
    "        return random.choice(valid_actions)  # PRandom policy selects randomly among valid actions\n",
    "\n",
    "    def perform_action(self, action):\n",
    "        if action is None:\n",
    "            return\n",
    "        move_delta = {'N': (-1, 0), 'S': (1, 0), 'E': (0, 1), 'W': (0, -1)}\n",
    "        x, y = self.position\n",
    "        x += move_delta.get(action, (0, 0))[0]\n",
    "        y += move_delta.get(action, (0, 0))[1]\n",
    "        if action == 'P' and (x, y) in PICKUP_LOCATIONS:\n",
    "            self.carrying = True\n",
    "            PICKUP_LOCATIONS[(x, y)] -= 1\n",
    "        elif action == 'D' and (x, y) in DROPOFF_LOCATIONS:\n",
    "            self.carrying = False\n",
    "            DROPOFF_LOCATIONS[(x, y)] += 1\n",
    "        self.position = (x, y)\n",
    "\n",
    "    def update_q_table(self, action, reward, next_position):\n",
    "        if action is None:\n",
    "            return\n",
    "        old_q = self.q_table[self.position[0], self.position[1], ACTIONS.index(action)]\n",
    "        next_max_q = np.max(self.q_table[next_position[0], next_position[1]])\n",
    "        self.q_table[self.position[0], self.position[1], ACTIONS.index(action)] = \\\n",
    "            (1 - LEARNING_RATE) * old_q + LEARNING_RATE * (reward + DISCOUNT_FACTOR * next_max_q)\n",
    "\n",
    "    def is_terminal_state(self):\n",
    "        return all(value == 5 for value in DROPOFF_LOCATIONS.values())\n",
    "\n",
    "    def reset(self):\n",
    "        self.position = initial_positions[self.name]\n",
    "        self.carrying = False\n",
    "        for loc in PICKUP_LOCATIONS:\n",
    "            PICKUP_LOCATIONS[loc] = 5\n",
    "        for loc in DROPOFF_LOCATIONS:\n",
    "            DROPOFF_LOCATIONS[loc] = 0\n",
    "\n",
    "def run_experiment(agents, policy, steps):\n",
    "    for step in range(steps):\n",
    "        for agent in agents:\n",
    "            valid_actions = agent.get_valid_actions()\n",
    "            action = agent.select_action(valid_actions, policy)\n",
    "            agent.perform_action(action)\n",
    "            reward = ACTION_REWARDS.get(action, 0)\n",
    "            agent.update_q_table(action, reward, agent.position)\n",
    "            if agent.is_terminal_state():\n",
    "                agent.reset()\n",
    "\n",
    "# Initialize agents\n",
    "initial_positions = {'red': (2, 2), 'blue': (4, 2), 'black': (0, 2)}\n",
    "agents = [Agent(initial_positions[name], name) for name in initial_positions]\n",
    "\n",
    "# Run Experiments\n",
    "run_experiment(agents, 'PRandom', STEPS_PRANDOM_INITIAL)  # Initial random phase\n",
    "run_experiment(agents, 'PRandom', STEPS_PRANDOM_EXTENDED)  # Continue with PRandom\n",
    "\n",
    "# Output results\n",
    "for agent in agents:\n",
    "    print(f\"{agent.name}'s Q-table after Experiment 1a:\")\n",
    "    print(\"State  |   N       E       S       W       P       D\")\n",
    "    print(\"---------------------------------------------------\")\n",
    "    for x in range(GRID_SIZE):\n",
    "        for y in range(GRID_SIZE):\n",
    "            q_values = agent.q_table[x, y]\n",
    "            print(f\"({x+1},{y+1})  |  \" + \"  \".join(f\"{q_value:6.2f}\" for q_value in q_values))\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e98667-d987-4108-9314-646ab751f558",
   "metadata": {},
   "source": [
    "<h3> EXP1 B.GREEDY=8500</h3> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47618b1d-f84f-4c47-a176-b67d4f7d3c04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "red's Q-table after Experiment 1b:\n",
      "State  |   N       E       S       W       P       D\n",
      "---------------------------------------------------\n",
      "(1,1)  |   12.00    0.00    0.00   -0.30   26.00   26.00\n",
      "(1,2)  |    0.28    0.28    0.00   -0.07    4.48    3.90\n",
      "(1,3)  |   -0.25   -0.17    0.00    0.16    4.48    3.90\n",
      "(1,4)  |    0.00   -0.51    0.00    0.00    4.48    7.30\n",
      "(1,5)  |    0.00    0.00    0.00    0.00    0.00    0.00\n",
      "(2,1)  |   12.00    0.00   12.00    0.00   26.00   26.00\n",
      "(2,2)  |    1.40    0.28    0.00    0.99    5.40   10.03\n",
      "(2,3)  |    1.33    3.02    3.45    0.00    8.91   12.49\n",
      "(2,4)  |    0.78   -0.30    0.00    0.98    7.21    0.00\n",
      "(2,5)  |    0.43   -0.51    0.00    0.00    0.00    7.21\n",
      "(3,1)  |   12.00    0.00   12.00    0.07   26.00   26.00\n",
      "(3,2)  |    0.00    0.37   -0.30    0.38    3.90    4.48\n",
      "(3,3)  |    1.77    0.63    2.38    2.47   10.11    8.56\n",
      "(3,4)  |    1.71    0.98    0.57    1.33    0.00    7.21\n",
      "(3,5)  |    1.22   -0.30    0.86    0.00   10.11    4.48\n",
      "(4,1)  |   12.00    0.00   12.00    1.18   26.00   26.00\n",
      "(4,2)  |   -0.66    0.00    0.00   -0.30    0.00    0.00\n",
      "(4,3)  |    0.28    0.00    1.29    0.98    0.00    7.21\n",
      "(4,4)  |    0.28   -0.07    0.48    0.48    0.00    3.90\n",
      "(4,5)  |   -0.30   -0.51    0.00    0.00    0.00    0.00\n",
      "(5,1)  |    0.00    0.00   12.00    3.53   26.00   26.00\n",
      "(5,2)  |    0.00    3.15    1.15    3.15   16.60   13.91\n",
      "(5,3)  |    0.00   -0.51    0.07    0.00    0.00    3.90\n",
      "(5,4)  |    0.00   -0.30   -0.30    0.00    0.00    0.00\n",
      "(5,5)  |    0.00   -0.30    0.00    0.00    0.00    0.00\n",
      "\n",
      "\n",
      "blue's Q-table after Experiment 1b:\n",
      "State  |   N       E       S       W       P       D\n",
      "---------------------------------------------------\n",
      "(1,1)  |    0.00    0.00    0.00   12.00   26.00   26.00\n",
      "(1,2)  |    0.00   12.00    0.00   12.00   26.00   26.00\n",
      "(1,3)  |    0.00   12.00    0.00   12.00   26.00   26.00\n",
      "(1,4)  |   -0.30   12.00    0.00   12.00   26.00   26.00\n",
      "(1,5)  |    0.00   12.00    0.00    0.00   26.00   26.00\n",
      "(2,1)  |   -0.30    0.00    0.00    0.00    0.00    0.00\n",
      "(2,2)  |   -0.30   -0.30    0.00    0.00    0.00    0.00\n",
      "(2,3)  |    0.00   -0.30    0.00    0.00    0.00    0.00\n",
      "(2,4)  |   -0.30   -0.30    0.00   -0.30    0.00    0.00\n",
      "(2,5)  |   -0.30    0.00    0.00    0.00    0.00    0.00\n",
      "(3,1)  |   -0.66    0.00    0.00    0.00    0.00    0.00\n",
      "(3,2)  |    1.33    0.00   -0.30    0.57    0.00    7.21\n",
      "(3,3)  |    0.57    0.28    0.00    0.28    7.21    0.00\n",
      "(3,4)  |    0.16    0.71    0.07    0.00    7.30    8.13\n",
      "(3,5)  |   -0.51    0.00    0.00    0.00    0.00    0.00\n",
      "(4,1)  |    1.20    0.00    0.57   -0.07    9.19   10.03\n",
      "(4,2)  |   -0.17    2.07    3.52    1.57   14.18   12.49\n",
      "(4,3)  |    0.78    1.26    0.00    2.11   12.43    4.98\n",
      "(4,4)  |    1.02    0.00    3.00    0.77   10.23   11.13\n",
      "(4,5)  |    0.28    0.23   -0.30    0.00    3.90    4.48\n",
      "(5,1)  |    0.00    0.00    1.20    2.09   17.66    5.40\n",
      "(5,2)  |    0.00    5.42    3.12    4.53   18.60   19.13\n",
      "(5,3)  |    0.00    1.01    2.05   -0.30   11.10   16.60\n",
      "(5,4)  |    0.00    0.07   -0.30    0.00    3.90    0.00\n",
      "(5,5)  |    0.00    0.00   -0.30    0.00    0.00    0.00\n",
      "\n",
      "\n",
      "black's Q-table after Experiment 1b:\n",
      "State  |   N       E       S       W       P       D\n",
      "---------------------------------------------------\n",
      "(1,1)  |   -0.30    0.00    0.00   12.00   26.00   26.00\n",
      "(1,2)  |    0.00   12.00    0.00   12.00   26.00   26.00\n",
      "(1,3)  |    0.00   12.00    0.00   12.00   26.00   26.00\n",
      "(1,4)  |   -0.30   12.00    0.00   12.00   26.00   26.00\n",
      "(1,5)  |    0.00   12.00    0.00    0.00   26.00   26.00\n",
      "(2,1)  |    0.00    0.00   -0.30   -0.51    0.00    0.00\n",
      "(2,2)  |    0.00    0.48   -0.30    0.34    7.21    4.98\n",
      "(2,3)  |    0.78    0.75    0.00    0.00    5.40   10.03\n",
      "(2,4)  |    0.00   -0.30    0.00    0.00    0.00    0.00\n",
      "(2,5)  |    0.00    0.00    0.00    0.00    0.00    0.00\n",
      "(3,1)  |   -0.30    0.00    0.00    0.00    0.00    0.00\n",
      "(3,2)  |   -0.30   -0.30    0.00   -0.66    0.00    0.00\n",
      "(3,3)  |    0.00   -0.51   -0.30   -0.30    3.90    7.71\n",
      "(3,4)  |    0.00    0.00    0.00   -0.30    0.00    0.00\n",
      "(3,5)  |   -0.30    0.00    0.00    0.00    0.00    0.00\n",
      "(4,1)  |    0.23    0.00    0.00    1.26    4.98   10.03\n",
      "(4,2)  |    0.49    1.53    0.49    0.57    4.48    7.30\n",
      "(4,3)  |    0.00    0.07    0.00   -0.30    3.90    0.00\n",
      "(4,4)  |   -0.30   -0.30    0.00    0.00    0.00    0.00\n",
      "(4,5)  |   -0.30   -0.30    0.00    0.00    3.90    0.00\n",
      "(5,1)  |    0.00    0.00    2.01    3.30   12.79   13.91\n",
      "(5,2)  |    0.00    1.76    1.72    0.78   12.43    5.40\n",
      "(5,3)  |    0.00   -0.51    0.00    0.00    0.00    3.90\n",
      "(5,4)  |    0.00   -0.30    0.00    0.00    0.00    0.00\n",
      "(5,5)  |    0.00    0.00   -0.30    0.00    0.00    0.00\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Environment setup\n",
    "GRID_SIZE = 5\n",
    "PICKUP_LOCATIONS = {(0, 4): 5, (1, 3): 5, (4, 1): 5}\n",
    "DROPOFF_LOCATIONS = {(0, 0): 0, (2, 0): 0, (3, 4): 0}\n",
    "ACTIONS = ['N', 'E', 'S', 'W', 'P', 'D']\n",
    "ACTION_REWARDS = {'P': 13, 'D': 13, 'N': -1, 'E': -1, 'S': -1, 'W': -1}\n",
    "\n",
    "# Hyperparameters\n",
    "LEARNING_RATE = 0.3\n",
    "DISCOUNT_FACTOR = 0.5\n",
    "STEPS_PRANDOM = 500  # Initial random steps\n",
    "STEPS_PGREEDY = 8500  # Steps for PGreedy policy\n",
    "\n",
    "# Seed for reproducibility\n",
    "random.seed(5)\n",
    "np.random.seed(5)\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, start_position, name):\n",
    "        self.position = start_position\n",
    "        self.name = name\n",
    "        self.carrying = False\n",
    "        self.q_table = np.zeros((GRID_SIZE, GRID_SIZE, len(ACTIONS)))\n",
    "\n",
    "    def get_valid_actions(self):\n",
    "        x, y = self.position\n",
    "        valid_actions = ACTIONS.copy()\n",
    "        if x == 0: valid_actions.remove('N')\n",
    "        if x == GRID_SIZE - 1: valid_actions.remove('S')\n",
    "        if y == 0: valid_actions.remove('W')\n",
    "        if y == GRID_SIZE - 1: valid_actions.remove('E')\n",
    "        if self.carrying and (x, y) in DROPOFF_LOCATIONS:\n",
    "            valid_actions.append('D')\n",
    "        elif not self.carrying and (x, y) in PICKUP_LOCATIONS and PICKUP_LOCATIONS[(x, y)] > 0:\n",
    "            valid_actions.append('P')\n",
    "        return valid_actions\n",
    "\n",
    "    def select_action(self, valid_actions, policy):\n",
    "        if not valid_actions:\n",
    "            return None\n",
    "        if policy == 'PGreedy':\n",
    "            q_values = [self.q_table[x, y, ACTIONS.index(a)] for a in valid_actions]\n",
    "            max_value = max(q_values)\n",
    "            best_actions = [a for a, q in zip(valid_actions, q_values) if q == max_value]\n",
    "            return random.choice(best_actions)\n",
    "\n",
    "    def perform_action(self, action):\n",
    "        if action is None:\n",
    "            return\n",
    "        move_delta = {'N': (-1, 0), 'S': (1, 0), 'E': (0, 1), 'W': (0, -1)}\n",
    "        x, y = self.position\n",
    "        x += move_delta.get(action, (0, 0))[0]\n",
    "        y += move_delta.get(action, (0, 0))[1]\n",
    "        if action == 'P' and (x, y) in PICKUP_LOCATIONS:\n",
    "            self.carrying = True\n",
    "            PICKUP_LOCATIONS[(x, y)] -= 1\n",
    "        elif action == 'D' and (x, y) in DROPOFF_LOCATIONS:\n",
    "            self.carrying = False\n",
    "            DROPOFF_LOCATIONS[(x, y)] += 1\n",
    "        self.position = (x, y)\n",
    "\n",
    "    def update_q_table(self, action, reward, next_position):\n",
    "        if action is None:\n",
    "            return\n",
    "        old_q = self.q_table[self.position[0], self.position[1], ACTIONS.index(action)]\n",
    "        next_max_q = np.max(self.q_table[next_position[0], next_position[1]])\n",
    "        self.q_table[self.position[0], self.position[1], ACTIONS.index(action)] = \\\n",
    "            (1 - LEARNING_RATE) * old_q + LEARNING_RATE * (reward + DISCOUNT_FACTOR * next_max_q)\n",
    "\n",
    "    def is_terminal_state(self):\n",
    "        return all(value == 5 for value in DROPOFF_LOCATIONS.values())\n",
    "\n",
    "    def reset(self):\n",
    "        self.position = initial_positions[self.name]\n",
    "        self.carrying = False\n",
    "        for loc in PICKUP_LOCATIONS:\n",
    "            PICKUP_LOCATIONS[loc] = 5\n",
    "        for loc in DROPOFF_LOCATIONS:\n",
    "            DROPOFF_LOCATIONS[loc] = 0\n",
    "\n",
    "def run_experiment(agents, policy, steps):\n",
    "    for step in range(steps):\n",
    "        for agent in agents:\n",
    "            valid_actions = agent.get_valid_actions()\n",
    "            action = agent.select_action(valid_actions, policy)\n",
    "            agent.perform_action(action)\n",
    "            reward = ACTION_REWARDS.get(action, 0)\n",
    "            agent.update_q_table(action, reward, agent.position)\n",
    "            if agent.is_terminal_state():\n",
    "                agent.reset()\n",
    "\n",
    "# Initialize agents\n",
    "initial_positions = {'red': (2, 2), 'blue': (4, 2), 'black': (0, 2)}\n",
    "agents = [Agent(initial_positions[name], name) for name in initial_positions]\n",
    "\n",
    "# Run Experiments\n",
    "run_experiment(agents, 'PRandom', STEPS_PRANDOM)  # Initial random phase\n",
    "run_experiment(agents, 'PGreedy', STEPS_PGREEDY)  # Experiment 1b\n",
    "\n",
    "# Output results\n",
    "for agent in agents:\n",
    "    print(f\"{agent.name}'s Q-table after Experiment 1b:\")\n",
    "    print(\"State  |   N       E       S       W       P       D\")\n",
    "    print(\"---------------------------------------------------\")\n",
    "    for x in range(GRID_SIZE):\n",
    "        for y in range(GRID_SIZE):\n",
    "            q_values = agent.q_table[x, y]\n",
    "            print(f\"({x+1},{y+1})  |  \" + \"  \".join(f\"{q_value:6.2f}\" for q_value in q_values))\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f058ff3f-2028-4dd0-9177-fc32f852a9e7",
   "metadata": {},
   "source": [
    "<h3> EXP1 C.Exploit =8500</h3> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d033fb14-75d7-47cf-a949-01d05f24332d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "red's Q-table after Experiment 1c:\n",
      "State  |   N       E       S       W       P       D\n",
      "---------------------------------------------------\n",
      "(1,1)  |    2.26    0.00    0.00    2.66   12.43   12.24\n",
      "(1,2)  |    2.16    3.06    0.00    2.12   16.45   15.28\n",
      "(1,3)  |    5.17    3.46    0.00    4.22    9.25   17.66\n",
      "(1,4)  |    4.30    4.81    0.00    5.18   13.55   20.31\n",
      "(1,5)  |    0.97    1.95    0.00    0.00   14.77    8.20\n",
      "(2,1)  |   -0.66    0.00    1.12    0.00    4.98    7.21\n",
      "(2,2)  |   -0.30    0.07   -0.66   -0.17    0.00    3.90\n",
      "(2,3)  |    1.26    1.41    1.02    0.57   10.03    0.00\n",
      "(2,4)  |    2.98    2.95    4.01    0.99   14.46    5.40\n",
      "(2,5)  |   -0.33    0.44   -0.07    0.00    4.48   10.11\n",
      "(3,1)  |   -0.66    0.00   -0.30   -0.76    0.00    0.00\n",
      "(3,2)  |    1.40    0.98    0.78    1.93   12.43    0.00\n",
      "(3,3)  |    3.42    1.37    2.70    1.33   11.10   10.62\n",
      "(3,4)  |    3.66    3.48    3.30    6.61    4.98   18.92\n",
      "(3,5)  |    7.35    1.63    6.52    0.00   26.00   20.88\n",
      "(4,1)  |    1.22    0.00    0.62    0.52    0.00    7.21\n",
      "(4,2)  |    1.26    3.71   -0.51    2.31    9.80   12.43\n",
      "(4,3)  |    7.32    6.38    3.61    7.68   20.45   21.12\n",
      "(4,4)  |    6.88    5.02    7.15    7.15   19.10    9.31\n",
      "(4,5)  |    3.13    4.24    2.96    0.00    7.21   16.85\n",
      "(5,1)  |    0.00    0.00    2.05    2.93    0.00   14.46\n",
      "(5,2)  |    0.00    0.85    2.09    0.83   10.03   11.63\n",
      "(5,3)  |    0.00    2.07    4.64    1.61   14.46    9.85\n",
      "(5,4)  |    0.00    3.64    1.86    1.33   11.36   14.75\n",
      "(5,5)  |    0.00   -0.09   -0.30    0.00    4.48    3.90\n",
      "\n",
      "\n",
      "blue's Q-table after Experiment 1c:\n",
      "State  |   N       E       S       W       P       D\n",
      "---------------------------------------------------\n",
      "(1,1)  |    0.00    0.00    0.00    0.00    0.00    0.00\n",
      "(1,2)  |    0.86    0.00    0.00    1.31    3.90    7.71\n",
      "(1,3)  |    0.82    1.48    0.00    0.07   10.11   11.41\n",
      "(1,4)  |   -0.07    0.07    0.00    0.34    0.00    3.90\n",
      "(1,5)  |    4.42    2.05    0.00    0.00   15.57   15.77\n",
      "(2,1)  |    2.44    0.00    0.00    1.89   14.46    8.89\n",
      "(2,2)  |   -0.30   -0.51   -0.51    0.37    4.48    3.90\n",
      "(2,3)  |    1.31   -0.30   -0.51    1.88    3.90    7.71\n",
      "(2,4)  |    3.16    3.87    4.07    4.25   20.30   18.10\n",
      "(2,5)  |    6.06    5.72    4.90    0.00   18.57   16.95\n",
      "(3,1)  |    3.55    0.00    3.40    4.78   20.29   18.07\n",
      "(3,2)  |    0.80    0.32    0.37    0.00    4.48    3.90\n",
      "(3,3)  |   -0.17   -0.51   -0.30    0.07    0.00    3.90\n",
      "(3,4)  |    3.43    0.28    2.07    4.83   16.19   12.20\n",
      "(3,5)  |    3.26    3.62    5.65    0.00   14.72   16.19\n",
      "(4,1)  |    6.40    0.00    4.39    6.22   19.36   21.13\n",
      "(4,2)  |    6.47    6.96    5.64    5.26   20.88   16.22\n",
      "(4,3)  |    1.20    4.31    3.70    4.38   17.84   18.08\n",
      "(4,4)  |    2.42    3.88    6.66    4.31   17.57   19.29\n",
      "(4,5)  |    0.86    0.75    1.06    0.00    7.79    7.71\n",
      "(5,1)  |    0.00    0.00    7.62    5.30   22.32   21.76\n",
      "(5,2)  |    0.00    7.43    3.39    6.57   26.00   14.50\n",
      "(5,3)  |    0.00    3.26    2.24    2.16   12.79    8.20\n",
      "(5,4)  |    0.00   -0.07    0.00    0.07    3.90    4.48\n",
      "(5,5)  |    0.00   -0.51   -0.30    0.00    4.48    3.90\n",
      "\n",
      "\n",
      "black's Q-table after Experiment 1c:\n",
      "State  |   N       E       S       W       P       D\n",
      "---------------------------------------------------\n",
      "(1,1)  |    0.28    0.00    0.00   -0.07    3.90    0.00\n",
      "(1,2)  |    1.27    1.12    0.00    0.91    3.90   10.46\n",
      "(1,3)  |    0.38    0.37    0.00    0.38    4.48    7.30\n",
      "(1,4)  |    1.91    3.31    0.00    4.79   18.58   18.69\n",
      "(1,5)  |    5.10    5.08    0.00    0.00   20.88    6.07\n",
      "(2,1)  |    1.90    0.00    1.41    1.30   12.79    8.20\n",
      "(2,2)  |    0.28    0.63    0.16    1.64    3.90    7.71\n",
      "(2,3)  |    1.06    1.54    0.57    1.73    7.71   10.52\n",
      "(2,4)  |    1.21    2.50    0.62    0.34   12.69   11.41\n",
      "(2,5)  |    4.38    2.08    1.93    0.00    4.48   14.52\n",
      "(3,1)  |    3.94    0.00    1.96    2.55   10.80   18.92\n",
      "(3,2)  |    0.48    0.57    0.48    0.00    7.21    4.98\n",
      "(3,3)  |    0.73    0.78   -0.30    1.33    0.00    7.21\n",
      "(3,4)  |    1.18    1.71    1.12    1.82    7.21    4.98\n",
      "(3,5)  |    3.98    4.03    3.08    0.00   15.11   17.66\n",
      "(4,1)  |    3.02    0.00    3.45    2.85   16.19    9.55\n",
      "(4,2)  |    1.72    2.86    0.65    2.05    7.79   10.47\n",
      "(4,3)  |    0.57    1.28    1.26    1.47    3.90   26.00\n",
      "(4,4)  |    9.30    6.62    6.46    7.47   21.72   21.86\n",
      "(4,5)  |    4.19    3.20    1.97    0.00   14.34   14.61\n",
      "(5,1)  |    0.00    0.00    2.10    3.23   12.79    3.90\n",
      "(5,2)  |    0.00    5.34    2.47    4.29   19.23   19.47\n",
      "(5,3)  |    0.00    3.03    1.78    1.37    8.47   10.22\n",
      "(5,4)  |    0.00    6.96    7.92    5.14   10.93   22.86\n",
      "(5,5)  |    0.00    6.98    7.10    0.00   19.15   22.87\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Define the environment setup\n",
    "GRID_SIZE = 5\n",
    "PICKUP_LOCATIONS = {(0, 4): 5, (1, 3): 5, (4, 1): 5}\n",
    "DROPOFF_LOCATIONS = {(0, 0): 0, (2, 0): 0, (3, 4): 0}\n",
    "ACTIONS = ['N', 'E', 'S', 'W', 'P', 'D']\n",
    "ACTION_REWARDS = {'P': 13, 'D': 13, 'N': -1, 'E': -1, 'S': -1, 'W': -1}\n",
    "\n",
    "# Hyperparameters\n",
    "LEARNING_RATE = 0.3\n",
    "DISCOUNT_FACTOR = 0.5\n",
    "STEPS_PRANDOM = 500  # Initial random steps\n",
    "STEPS_POLICY = 8500  # Steps for PExploit policy\n",
    "\n",
    "# Seed for reproducibility\n",
    "random.seed(4)\n",
    "np.random.seed(4)\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, start_position, name):\n",
    "        self.position = start_position\n",
    "        self.name = name\n",
    "        self.carrying = False\n",
    "        self.q_table = np.zeros((GRID_SIZE, GRID_SIZE, len(ACTIONS)))\n",
    "\n",
    "    def get_valid_actions(self):\n",
    "        x, y = self.position\n",
    "        valid_actions = ACTIONS.copy()\n",
    "        if x == 0: valid_actions.remove('N')\n",
    "        if x == GRID_SIZE - 1: valid_actions.remove('S')\n",
    "        if y == 0: valid_actions.remove('W')\n",
    "        if y == GRID_SIZE - 1: valid_actions.remove('E')\n",
    "        if self.carrying and (x, y) in DROPOFF_LOCATIONS:\n",
    "            valid_actions.append('D')\n",
    "        elif not self.carrying and (x, y) in PICKUP_LOCATIONS and PICKUP_LOCATIONS[(x, y)] > 0:\n",
    "            valid_actions.append('P')\n",
    "        return valid_actions\n",
    "\n",
    "    def select_action(self, valid_actions, policy):\n",
    "        if not valid_actions:\n",
    "            return None\n",
    "        if policy == 'PRandom':\n",
    "            return random.choice(valid_actions)\n",
    "        elif policy == 'PExploit':\n",
    "            q_values = [self.q_table[x, y, ACTIONS.index(a)] for a in valid_actions]\n",
    "            max_value = max(q_values)\n",
    "            best_actions = [a for a, q in zip(valid_actions, q_values) if q == max_value]\n",
    "            return random.choice(best_actions)\n",
    "\n",
    "    def perform_action(self, action):\n",
    "        if action is None:\n",
    "            return\n",
    "        move_delta = {'N': (-1, 0), 'S': (1, 0), 'E': (0, 1), 'W': (0, -1)}\n",
    "        x, y = self.position\n",
    "        x += move_delta.get(action, (0, 0))[0]\n",
    "        y += move_delta.get(action, (0, 0))[1]\n",
    "        if action == 'P' and (x, y) in PICKUP_LOCATIONS:\n",
    "            self.carrying = True\n",
    "            PICKUP_LOCATIONS[(x, y)] -= 1\n",
    "        elif action == 'D' and (x, y) in DROPOFF_LOCATIONS:\n",
    "            self.carrying = False\n",
    "            DROPOFF_LOCATIONS[(x, y)] += 1\n",
    "        self.position = (x, y)\n",
    "\n",
    "    def update_q_table(self, action, reward, next_position):\n",
    "        if action is None:\n",
    "            return\n",
    "        old_q = self.q_table[self.position[0], self.position[1], ACTIONS.index(action)]\n",
    "        next_max_q = np.max(self.q_table[next_position[0], next_position[1]])\n",
    "        self.q_table[self.position[0], self.position[1], ACTIONS.index(action)] = \\\n",
    "            (1 - LEARNING_RATE) * old_q + LEARNING_RATE * (reward + DISCOUNT_FACTOR * next_max_q)\n",
    "\n",
    "    def is_terminal_state(self):\n",
    "        return all(value == 5 for value in DROPOFF_LOCATIONS.values())\n",
    "\n",
    "    def reset(self):\n",
    "        self.position = initial_positions[self.name]\n",
    "        self.carrying = False\n",
    "        for loc in PICKUP_LOCATIONS:\n",
    "            PICKUP_LOCATIONS[loc] = 5\n",
    "        for loc in DROPOFF_LOCATIONS:\n",
    "            DROPOFF_LOCATIONS[loc] = 0\n",
    "\n",
    "initial_positions = {'red': (2, 2), 'blue': (4, 2), 'black': (0, 2)}\n",
    "agents = [Agent(initial_positions[name], name) for name in initial_positions]\n",
    "\n",
    "def run_experiment(agents, policy, steps):\n",
    "    for step in range(steps):\n",
    "        for agent in agents:\n",
    "            valid_actions = agent.get_valid_actions()\n",
    "            action = agent.select_action(valid_actions, policy)\n",
    "            agent.perform_action(action)\n",
    "            reward = ACTION_REWARDS.get(action, 0)\n",
    "            agent.update_q_table(action, reward, agent.position)\n",
    "            if agent.is_terminal_state():\n",
    "                agent.reset()\n",
    "\n",
    "# Run Experiments\n",
    "run_experiment(agents, 'PRandom', STEPS_PRANDOM)  # Initial random phase\n",
    "run_experiment(agents, 'PExploit', STEPS_POLICY)  # Experiment 1c\n",
    "\n",
    "# Output results\n",
    "for agent in agents:\n",
    "    print(f\"{agent.name}'s Q-table after Experiment 1c:\")\n",
    "    print(\"State  |   N       E       S       W       P       D\")\n",
    "    print(\"---------------------------------------------------\")\n",
    "    for x in range(GRID_SIZE):\n",
    "        for y in range(GRID_SIZE):\n",
    "            q_values = agent.q_table[x, y]\n",
    "            print(f\"({x+1},{y+1})  |  \" + \"  \".join(f\"{q_value:6.2f}\" for q_value in q_values))\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "8ba00a97-9b98-47ad-a521-bd125af7f1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class QTable:\n",
    "    def __init__(self, num_states, num_actions, learning_rate=0.1, discount_factor=0.9):\n",
    "        self.num_states = num_states\n",
    "        self.num_actions = num_actions\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.q_table = np.zeros((num_states, num_actions))\n",
    "\n",
    "    def update_q_value(self, state, action, reward, next_state):\n",
    "        current_q_value = self.q_table[state, action]\n",
    "        max_next_q_value = np.max(self.q_table[next_state])\n",
    "        new_q_value = (1 - self.learning_rate) * current_q_value + \\\n",
    "                      self.learning_rate * (reward + self.discount_factor * max_next_q_value)\n",
    "        self.q_table[state, action] = new_q_value\n",
    "\n",
    "    def get_q_value(self, state, action):\n",
    "        return self.q_table[state, action]\n",
    "\n",
    "    def update_learning_rate(self, new_learning_rate):\n",
    "        self.learning_rate = new_learning_rate\n",
    "\n",
    "    def update_discount_factor(self, new_discount_factor):\n",
    "        self.discount_factor = new_discount_factor\n",
    "\n",
    "    def get_optimal_action(self, state, exploration_rate):\n",
    "        if np.random.rand() < exploration_rate:\n",
    "            return np.random.randint(self.num_actions)  # Randomly choose an action\n",
    "        else:\n",
    "            return np.argmax(self.q_table[state])  # Choose action with maximum Q-value\n",
    "\n",
    "class BlockTransportationProblem:\n",
    "    def __init__(self, num_agents, num_states, num_actions, learning_rate, discount_factor):\n",
    "        self.num_agents = num_agents\n",
    "        self.num_states = num_states\n",
    "        self.num_actions = num_actions\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.q_tables = [QTable(num_states, num_actions, learning_rate, discount_factor) for _ in range(num_agents)]\n",
    "\n",
    "    def update_q_values(self, agent_index, state, action, reward, next_state):\n",
    "        self.q_tables[agent_index].update_q_value(state, action, reward, next_state)\n",
    "\n",
    "    def select_action(self, agent_index, state, exploration_rate, policy):\n",
    "        if policy == \"PRANDOM\":\n",
    "            return np.random.randint(self.num_actions)  # Choose action randomly\n",
    "        elif policy == \"PEXPLOIT\":\n",
    "            return self.q_tables[agent_index].get_optimal_action(state, exploration_rate)\n",
    "        elif policy == \"PGREEDY\":\n",
    "            return np.argmax(self.q_tables[agent_index].q_table[state])  # Choose action with maximum Q-value\n",
    "\n",
    "# Experiment parameters\n",
    "num_agents = 3\n",
    "num_states = 25  # Assuming 25 states\n",
    "num_actions = 4  # Assuming 4 actions (north, south, east, west)\n",
    "learning_rate = 0.3\n",
    "discount_factor = 0.5\n",
    "total_steps = 9000\n",
    "initial_prandom_steps = 500\n",
    "remaining_steps = total_steps - initial_prandom_steps\n",
    "policy_switch_step = initial_prandom_steps\n",
    "\n",
    "# Initialize Block Transportation Problem\n",
    "btp = BlockTransportationProblem(num_agents, num_states, num_actions,learning_rate,discount_factor)\n",
    "\n",
    "# Run PRANDOM for initial steps\n",
    "for step in range(initial_prandom_steps):\n",
    "    states = [np.random.randint(num_states) for _ in range(num_agents)]\n",
    "    actions = [btp.select_action(agent_index, state, exploration_rate=1.0, policy=\"PRANDOM\") for agent_index, state in enumerate(states)]\n",
    "    rewards = [np.random.randint(10) for _ in range(num_agents)]  # Random rewards for demonstration\n",
    "    next_states = [np.random.randint(num_states) for _ in range(num_agents)]\n",
    "    for agent_index in range(num_agents):\n",
    "        btp.update_q_values(agent_index, states[agent_index], actions[agent_index], rewards[agent_index], next_states[agent_index])\n",
    "\n",
    "# Switch policies and continue running\n",
    "for step in range(policy_switch_step, total_steps):\n",
    "    policy = \"PGREEDY\" if step < (total_steps - remaining_steps // 2) else \"PEXPLOIT\"\n",
    "    exploration_rate = 0.1 if policy == \"PEXPLOIT\" else 1.0\n",
    "    states = [np.random.randint(num_states) for _ in range(num_agents)]\n",
    "    actions = [btp.select_action(agent_index, state, exploration_rate, policy) for agent_index, state in enumerate(states)]\n",
    "    rewards = [np.random.randint(10) for _ in range(num_agents)]  # Random rewards for demonstration\n",
    "    next_states = [np.random.randint(num_states) for _ in range(num_agents)]\n",
    "    for agent_index in range(num_agents):\n",
    "        btp.update_q_values(agent_index, states[agent_index], actions[agent_index], rewards[agent_index], next_states[agent_index])\n",
    "\n",
    "final_states = [np.random.randint(num_states) for _ in range(num_agents)]\n",
    "\n",
    "policy = \"PEXPLOIT\"\n",
    "final_q_table = btp.q_tables[0].q_table  # Assuming agents have the same Q-table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "5148686f-15b8-4048-9bb0-32ab186f1497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 7.00503694  7.78732465  8.59852831  8.60352702]\n",
      " [ 6.05355659  6.9877843   6.61093801  8.45854186]\n",
      " [11.69697107  6.85421518  5.51693656  7.12918729]\n",
      " [ 8.67679623  6.30499648  7.90454484  7.76605359]\n",
      " [ 6.5740658   6.14716011  6.70127726  7.89620298]\n",
      " [ 7.20960279  7.07304751  6.12619347  7.89538157]\n",
      " [ 5.71143053  7.4342129   8.68682991 10.22936457]\n",
      " [ 8.11074145  8.49385569  6.93596339  7.83008148]\n",
      " [ 7.22462064  7.54871298  7.67656901 10.45340967]\n",
      " [ 7.2511047   9.97652313  6.76792038  6.25029643]\n",
      " [ 5.11537363  9.62588791  6.95030787  6.67769375]\n",
      " [ 7.95021029 11.15103703  7.57058169  7.42208313]\n",
      " [ 6.3392159   8.66268703  6.88790682 10.38479038]\n",
      " [ 5.55388604  8.64765435  6.41312333  6.93900456]\n",
      " [ 9.64091392  6.97437254  6.79949482  6.67494951]\n",
      " [ 7.31530805  7.43805912  7.21960983  9.33020618]\n",
      " [ 7.75194522  5.0191504   8.15040355 11.2883345 ]\n",
      " [ 6.49864004  6.59574904  8.89215252  7.03100242]\n",
      " [ 9.47166001  6.83349677  7.67398663  7.73384051]\n",
      " [ 6.6392951   5.53516481  6.41064401  6.92257444]\n",
      " [ 7.4389793   6.36655476  8.13130394 10.80161594]\n",
      " [ 6.97062044  8.62683632  5.85660049  5.92231136]\n",
      " [ 9.87419543  7.25829387  7.45759636  7.65917162]\n",
      " [ 6.98595009  6.99237905  8.44257072  7.62612004]\n",
      " [ 6.62327118  7.32806208  7.48846358  9.33196804]]\n"
     ]
    }
   ],
   "source": [
    "print(final_q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca4ca36-38b1-410d-a9a5-df3d21b1d8e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ab359a-84e8-4ec3-83c5-c0d5f71f4f3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "bf4385d7-aa9a-4a84-8cab-740461994bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Q-Table for Experiment 2 (SARSA):\n",
      "[[ 7.69489145  7.30731207  9.15917982  7.20759023]\n",
      " [ 7.70551939  7.83009582 10.18136422  7.69689014]\n",
      " [ 6.43137058  5.36012178  6.84581668  9.67628003]\n",
      " [ 6.92972656  7.88601297  6.96209785  9.64069805]\n",
      " [ 5.91817267  7.74922127  6.53013358  7.04838564]\n",
      " [ 7.17631581  7.85516956  6.86019335  8.90321939]\n",
      " [11.49593275  7.18641286  6.91201564  9.00027687]\n",
      " [ 7.21555706  7.23357596  8.13285405  6.9126108 ]\n",
      " [ 7.56162342  9.21199453  7.48434565  6.56246388]\n",
      " [ 7.51717167  6.8564807  10.24051227  6.74161605]\n",
      " [ 8.06489509  5.82176862  7.33390791  6.44948564]\n",
      " [ 6.04554829  9.9207904   5.757327    5.67044394]\n",
      " [ 8.55599842  7.77909734  6.57297255  6.0612052 ]\n",
      " [ 7.14260543  6.63142825  6.90321385  9.29520028]\n",
      " [ 8.00602835  7.52976584  6.29494984  6.65833033]\n",
      " [ 6.42009631  9.19182138  6.48752771  7.14018259]\n",
      " [ 6.31456673 10.28249488  6.02792077  7.12299293]\n",
      " [ 6.07957032  5.58712984  8.43679549  9.65963799]\n",
      " [ 6.40197019  7.41625154  7.87675334 10.24238462]\n",
      " [ 5.57533708  8.52995441  8.06605063 10.10894364]\n",
      " [ 8.81383583  7.06674466 10.02070839  6.43504143]\n",
      " [ 7.26087141  7.73081342  8.41468062  9.85441903]\n",
      " [ 7.60801829  7.0078761   7.72362859  8.33538144]\n",
      " [ 6.10626309  6.36235598  6.79695403  9.52255186]\n",
      " [ 5.81164001  4.38120892  7.17969316  7.95165599]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class QTableSARSA:\n",
    "    def __init__(self, num_states, num_actions, learning_rate=0.1, discount_factor=0.9):\n",
    "        self.num_states = num_states\n",
    "        self.num_actions = num_actions\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.q_table = np.zeros((num_states, num_actions))\n",
    "\n",
    "    def update_q_value(self, state, action, reward, next_state, next_action):\n",
    "        current_q_value = self.q_table[state, action]\n",
    "        next_q_value = self.q_table[next_state, next_action]\n",
    "        new_q_value = (1 - self.learning_rate) * current_q_value + \\\n",
    "                      self.learning_rate * (reward + self.discount_factor * next_q_value)\n",
    "        self.q_table[state, action] = new_q_value\n",
    "\n",
    "    def get_q_value(self, state, action):\n",
    "        return self.q_table[state, action]\n",
    "\n",
    "    def update_learning_rate(self, new_learning_rate):\n",
    "        self.learning_rate = new_learning_rate\n",
    "\n",
    "    def update_discount_factor(self, new_discount_factor):\n",
    "        self.discount_factor = new_discount_factor\n",
    "\n",
    "    def get_optimal_action(self, state, exploration_rate):\n",
    "        if np.random.rand() < exploration_rate:\n",
    "            return np.random.randint(self.num_actions)  # Randomly choose an action\n",
    "        else:\n",
    "            return np.argmax(self.q_table[state])  # Choose action with maximum Q-value\n",
    "\n",
    "class BlockTransportationProblemSARSA:\n",
    "    def __init__(self, num_agents, num_states, num_actions, learning_rate, discount_factor):\n",
    "        self.num_agents = num_agents\n",
    "        self.num_states = num_states\n",
    "        self.num_actions = num_actions\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.q_tables = [QTableSARSA(num_states, num_actions, learning_rate, discount_factor) for _ in range(num_agents)]\n",
    "\n",
    "    def update_q_values(self, agent_index, state, action, reward, next_state, next_action):\n",
    "        self.q_tables[agent_index].update_q_value(state, action, reward, next_state, next_action)\n",
    "\n",
    "    def select_action(self, agent_index, state, exploration_rate, policy):\n",
    "        if policy == \"PRANDOM\":\n",
    "            return np.random.randint(self.num_actions)  # Choose action randomly\n",
    "        elif policy == \"PEXPLOIT\":\n",
    "            return self.q_tables[agent_index].get_optimal_action(state, exploration_rate)\n",
    "        elif policy == \"PGREEDY\":\n",
    "            return np.argmax(self.q_tables[agent_index].q_table[state])  # Choose action with maximum Q-value\n",
    "\n",
    "# Experiment parameters\n",
    "num_agents = 3\n",
    "num_states = 25  # Assuming 25 states\n",
    "num_actions = 4  # Assuming 4 actions (north, south, east, west)\n",
    "learning_rate = 0.3\n",
    "discount_factor = 0.5\n",
    "total_steps = 9000\n",
    "initial_prandom_steps = 500\n",
    "remaining_steps = total_steps - initial_prandom_steps\n",
    "policy_switch_step = initial_prandom_steps\n",
    "\n",
    "# Initialize Block Transportation Problem with SARSA\n",
    "btp_sarsa = BlockTransportationProblemSARSA(num_agents, num_states, num_actions, learning_rate, discount_factor)\n",
    "\n",
    "# Run PRANDOM for initial steps\n",
    "for step in range(initial_prandom_steps):\n",
    "    states = [np.random.randint(num_states) for _ in range(num_agents)]\n",
    "    actions = [btp_sarsa.select_action(agent_index, state, exploration_rate=1.0, policy=\"PRANDOM\") for agent_index, state in enumerate(states)]\n",
    "    rewards = [np.random.randint(10) for _ in range(num_agents)]  # Random rewards for demonstration\n",
    "    next_states = [np.random.randint(num_states) for _ in range(num_agents)]\n",
    "    next_actions = [btp_sarsa.select_action(agent_index, next_state, exploration_rate=0.1, policy=\"PEXPLOIT\") for agent_index, next_state in enumerate(next_states)]\n",
    "    for agent_index in range(num_agents):\n",
    "        btp_sarsa.update_q_values(agent_index, states[agent_index], actions[agent_index], rewards[agent_index], next_states[agent_index], next_actions[agent_index])\n",
    "\n",
    "# Switch policies and continue running\n",
    "for step in range(policy_switch_step, total_steps):\n",
    "    policy = \"PGREEDY\" if step < (total_steps - remaining_steps // 2) else \"PEXPLOIT\"\n",
    "    exploration_rate = 0.1 if policy == \"PEXPLOIT\" else 1.0\n",
    "    states = [np.random.randint(num_states) for _ in range(num_agents)]\n",
    "    actions = [btp_sarsa.select_action(agent_index, state, exploration_rate, policy) for agent_index, state in enumerate(states)]\n",
    "    rewards = [np.random.randint(10) for _ in range(num_agents)]  # Random rewards for demonstration\n",
    "    next_states = [np.random.randint(num_states) for _ in range(num_agents)]\n",
    "    next_actions = [btp_sarsa.select_action(agent_index, next_state, exploration_rate, policy) for agent_index, next_state in enumerate(next_states)]\n",
    "    for agent_index in range(num_agents):\n",
    "        btp_sarsa.update_q_values(agent_index, states[agent_index], actions[agent_index], rewards[agent_index], next_states[agent_index], next_actions[agent_index])\n",
    "\n",
    "# Report one of the final Q-tables\n",
    "final_q_table_sarsa = btp_sarsa.q_tables[0].q_table  # Assuming agents have the same Q-table\n",
    "print(\"Final Q-Table for Experiment 2 (SARSA):\")\n",
    "print(final_q_table_sarsa)\n",
    "\n",
    "# Assess the quality of agent coordination\n",
    "# You can assess coordination by analyzing metrics such as total rewards, convergence speed, and task completion rates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61b74dd-9ad6-4a9d-a0c9-e54dc78539ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4bb1ae5-9c23-4b63-a94b-134945ec493b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed06d70-5140-4254-a322-5de2e3934745",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99eadb28-ccab-4911-b6e3-4d0a867aa107",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8f39cb-0a3a-4ccc-ad59-68c9a0921e0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bb66914d-3a13-45c4-8be4-3ab311b6cc23",
   "metadata": {},
   "source": [
    "<h3> EXP2  SARSA Q-learning 9000 (500,8500) &report  Q-tables  </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e00de8-c747-4910-ae9d-612baa9cec11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a7a5b3f9-48d3-449d-a0f3-ea7e933d079d",
   "metadata": {},
   "source": [
    "<h3> EXP3 From 1.c (Qlearning) Different Learning rate (0.15,0.45)  </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38f9d377-0867-473a-b4c3-899cc2a9b4ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "red's Q-table after Experiment 1c:\n",
      "State  |   N       E       S       W       P       D\n",
      "---------------------------------------------------\n",
      "(1,1)  |    0.48    0.00    0.00    0.69    6.97    6.19\n",
      "(1,2)  |    0.49    0.81    0.00    0.32    9.81    8.11\n",
      "(1,3)  |    1.64    0.89    0.00    1.26    4.37   10.94\n",
      "(1,4)  |    1.14    1.48    0.00    1.63    7.32   13.27\n",
      "(1,5)  |    0.04    0.30    0.00    0.00    8.50    4.02\n",
      "(2,1)  |   -0.39    0.00    0.13    0.00    2.23    3.75\n",
      "(2,2)  |   -0.15   -0.13   -0.39   -0.33    0.00    1.95\n",
      "(2,3)  |    0.15    0.04    0.03    0.00    5.42    0.00\n",
      "(2,4)  |    0.74    0.69    1.13    0.13    8.39    2.36\n",
      "(2,5)  |   -0.53   -0.10   -0.24    0.00    2.10    5.43\n",
      "(3,1)  |   -0.39    0.00   -0.15   -0.48    0.00    0.00\n",
      "(3,2)  |    0.25    0.13    0.13    0.29    6.97    0.00\n",
      "(3,3)  |    0.77    0.16    0.62    0.24    5.77    5.57\n",
      "(3,4)  |    0.98    0.88    0.83    2.49    2.23   12.06\n",
      "(3,5)  |    2.87    0.09    2.30    0.00   26.00   14.08\n",
      "(4,1)  |    0.12    0.00   -0.01   -0.10    0.00    3.75\n",
      "(4,2)  |    0.15    0.97   -0.28    0.31    4.57    6.97\n",
      "(4,3)  |    2.80    2.07    1.03    3.01   12.91   14.17\n",
      "(4,4)  |    2.62    1.53    2.80    2.77   12.15    4.43\n",
      "(4,5)  |    0.75    1.07    0.58    0.00    3.75    9.99\n",
      "(5,1)  |    0.00    0.00    0.47    0.62    0.00    8.39\n",
      "(5,2)  |    0.00    0.02    0.38    0.02    5.42    5.97\n",
      "(5,3)  |    0.00    0.48    1.42    0.26    8.39    4.58\n",
      "(5,4)  |    0.00    0.93    0.27    0.24    5.87    8.45\n",
      "(5,5)  |    0.00   -0.32   -0.15    0.00    2.10    1.95\n",
      "\n",
      "\n",
      "blue's Q-table after Experiment 1c:\n",
      "State  |   N       E       S       W       P       D\n",
      "---------------------------------------------------\n",
      "(1,1)  |    0.00    0.00    0.00    0.00    0.00    0.00\n",
      "(1,2)  |    0.14    0.00    0.00    0.15    1.95    3.89\n",
      "(1,3)  |    0.02    0.26    0.00   -0.13    5.43    5.88\n",
      "(1,4)  |   -0.24   -0.13    0.00   -0.12    0.00    1.95\n",
      "(1,5)  |    1.18    0.47    0.00    0.00    8.77    8.94\n",
      "(2,1)  |    0.50    0.00    0.00    0.37    8.39    4.25\n",
      "(2,2)  |   -0.15   -0.28   -0.28    0.01    2.10    1.95\n",
      "(2,3)  |    0.15   -0.15   -0.28    0.36    1.95    3.89\n",
      "(2,4)  |    0.80    1.07    1.11    1.26   13.27   10.57\n",
      "(2,5)  |    2.05    1.81    1.48    0.00   11.35    9.97\n",
      "(3,1)  |    0.89    0.00    0.87    1.47   13.26   10.53\n",
      "(3,2)  |   -0.07   -0.20    0.01    0.00    2.10    1.95\n",
      "(3,3)  |   -0.33   -0.28   -0.15   -0.13    0.00    1.95\n",
      "(3,4)  |    0.87   -0.00    0.48    1.46    9.71    6.18\n",
      "(3,5)  |    0.79    1.07    1.83    0.00    7.85    9.71\n",
      "(4,1)  |    2.28    0.00    1.23    2.05   11.84   14.15\n",
      "(4,2)  |    2.26    2.76    1.90    1.67   14.08    8.58\n",
      "(4,3)  |    0.26    1.18    0.92    1.20   10.97   10.64\n",
      "(4,4)  |    0.68    0.99    2.59    1.27   10.36   12.16\n",
      "(4,5)  |    0.14   -0.20    0.14    0.00    3.90    3.89\n",
      "(5,1)  |    0.00    0.00    3.00    1.55   15.34   14.63\n",
      "(5,2)  |    0.00    2.91    0.94    2.33   26.00    7.76\n",
      "(5,3)  |    0.00    0.80    0.41    0.49    7.08    4.02\n",
      "(5,4)  |    0.00   -0.24    0.00   -0.13    1.95    2.10\n",
      "(5,5)  |    0.00   -0.28   -0.15    0.00    2.10    1.95\n",
      "\n",
      "\n",
      "black's Q-table after Experiment 1c:\n",
      "State  |   N       E       S       W       P       D\n",
      "---------------------------------------------------\n",
      "(1,1)  |   -0.00    0.00    0.00   -0.24    1.95    0.00\n",
      "(1,2)  |    0.27    0.15    0.00    0.03    1.95    5.55\n",
      "(1,3)  |   -0.19    0.01    0.00   -0.19    2.10    3.76\n",
      "(1,4)  |    0.39    0.74    0.00    1.38   11.28   11.42\n",
      "(1,5)  |    1.67    1.58    0.00    0.00   14.08    2.58\n",
      "(2,1)  |    0.29    0.00    0.25    0.15    7.08    4.02\n",
      "(2,2)  |   -0.00    0.01   -0.12    0.27    1.95    3.89\n",
      "(2,3)  |    0.14    0.27    0.00    0.19    3.89    5.56\n",
      "(2,4)  |    0.14    0.50   -0.01   -0.12    7.01    5.88\n",
      "(2,5)  |    1.32    0.48    0.37    0.00    2.10    8.40\n",
      "(3,1)  |    1.08    0.00    0.24    0.67    4.96   12.06\n",
      "(3,2)  |   -0.01    0.00   -0.01    0.00    3.75    2.23\n",
      "(3,3)  |   -0.07    0.13   -0.15    0.24    0.00    3.75\n",
      "(3,4)  |    0.13    0.34    0.13    0.22    3.75    2.23\n",
      "(3,5)  |    1.02    1.09    0.79    0.00    8.03   10.94\n",
      "(4,1)  |    0.64    0.00    0.87    0.69    9.71    4.48\n",
      "(4,2)  |    0.28    0.61    0.01    0.39    3.90    5.55\n",
      "(4,3)  |    0.00    0.07    0.15    0.09    1.95   26.00\n",
      "(4,4)  |    4.45    2.35    2.18    2.84   14.62   14.64\n",
      "(4,5)  |    1.15    0.69    0.38    0.00    7.68    8.42\n",
      "(5,1)  |    0.00    0.00    0.37    0.78    7.08    1.95\n",
      "(5,2)  |    0.00    1.62    0.54    1.24   11.78   12.26\n",
      "(5,3)  |    0.00    0.74    0.37    0.15    4.13    5.45\n",
      "(5,4)  |    0.00    2.64    3.19    1.60    5.08   16.56\n",
      "(5,5)  |    0.00    2.65    2.65    0.00   10.82   16.57\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Define the environment setup\n",
    "GRID_SIZE = 5\n",
    "PICKUP_LOCATIONS = {(0, 4): 5, (1, 3): 5, (4, 1): 5}\n",
    "DROPOFF_LOCATIONS = {(0, 0): 0, (2, 0): 0, (3, 4): 0}\n",
    "ACTIONS = ['N', 'E', 'S', 'W', 'P', 'D']\n",
    "ACTION_REWARDS = {'P': 13, 'D': 13, 'N': -1, 'E': -1, 'S': -1, 'W': -1}\n",
    "\n",
    "# Hyperparameters\n",
    "LEARNING_RATE = 0.15\n",
    "DISCOUNT_FACTOR = 0.5\n",
    "STEPS_PRANDOM = 500  # Initial random steps\n",
    "STEPS_POLICY = 8500  # Steps for PExploit policy\n",
    "\n",
    "# Seed for reproducibility\n",
    "random.seed(4)\n",
    "np.random.seed(4)\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, start_position, name):\n",
    "        self.position = start_position\n",
    "        self.name = name\n",
    "        self.carrying = False\n",
    "        self.q_table = np.zeros((GRID_SIZE, GRID_SIZE, len(ACTIONS)))\n",
    "\n",
    "    def get_valid_actions(self):\n",
    "        x, y = self.position\n",
    "        valid_actions = ACTIONS.copy()\n",
    "        if x == 0: valid_actions.remove('N')\n",
    "        if x == GRID_SIZE - 1: valid_actions.remove('S')\n",
    "        if y == 0: valid_actions.remove('W')\n",
    "        if y == GRID_SIZE - 1: valid_actions.remove('E')\n",
    "        if self.carrying and (x, y) in DROPOFF_LOCATIONS:\n",
    "            valid_actions.append('D')\n",
    "        elif not self.carrying and (x, y) in PICKUP_LOCATIONS and PICKUP_LOCATIONS[(x, y)] > 0:\n",
    "            valid_actions.append('P')\n",
    "        return valid_actions\n",
    "\n",
    "    def select_action(self, valid_actions, policy):\n",
    "        if not valid_actions:\n",
    "            return None\n",
    "        if policy == 'PRandom':\n",
    "            return random.choice(valid_actions)\n",
    "        elif policy == 'PExploit':\n",
    "            q_values = [self.q_table[x, y, ACTIONS.index(a)] for a in valid_actions]\n",
    "            max_value = max(q_values)\n",
    "            best_actions = [a for a, q in zip(valid_actions, q_values) if q == max_value]\n",
    "            return random.choice(best_actions)\n",
    "\n",
    "    def perform_action(self, action):\n",
    "        if action is None:\n",
    "            return\n",
    "        move_delta = {'N': (-1, 0), 'S': (1, 0), 'E': (0, 1), 'W': (0, -1)}\n",
    "        x, y = self.position\n",
    "        x += move_delta.get(action, (0, 0))[0]\n",
    "        y += move_delta.get(action, (0, 0))[1]\n",
    "        if action == 'P' and (x, y) in PICKUP_LOCATIONS:\n",
    "            self.carrying = True\n",
    "            PICKUP_LOCATIONS[(x, y)] -= 1\n",
    "        elif action == 'D' and (x, y) in DROPOFF_LOCATIONS:\n",
    "            self.carrying = False\n",
    "            DROPOFF_LOCATIONS[(x, y)] += 1\n",
    "        self.position = (x, y)\n",
    "\n",
    "    def update_q_table(self, action, reward, next_position):\n",
    "        if action is None:\n",
    "            return\n",
    "        old_q = self.q_table[self.position[0], self.position[1], ACTIONS.index(action)]\n",
    "        next_max_q = np.max(self.q_table[next_position[0], next_position[1]])\n",
    "        self.q_table[self.position[0], self.position[1], ACTIONS.index(action)] = \\\n",
    "            (1 - LEARNING_RATE) * old_q + LEARNING_RATE * (reward + DISCOUNT_FACTOR * next_max_q)\n",
    "\n",
    "    def is_terminal_state(self):\n",
    "        return all(value == 5 for value in DROPOFF_LOCATIONS.values())\n",
    "\n",
    "    def reset(self):\n",
    "        self.position = initial_positions[self.name]\n",
    "        self.carrying = False\n",
    "        for loc in PICKUP_LOCATIONS:\n",
    "            PICKUP_LOCATIONS[loc] = 5\n",
    "        for loc in DROPOFF_LOCATIONS:\n",
    "            DROPOFF_LOCATIONS[loc] = 0\n",
    "\n",
    "initial_positions = {'red': (2, 2), 'blue': (4, 2), 'black': (0, 2)}\n",
    "agents = [Agent(initial_positions[name], name) for name in initial_positions]\n",
    "\n",
    "def run_experiment(agents, policy, steps):\n",
    "    for step in range(steps):\n",
    "        for agent in agents:\n",
    "            valid_actions = agent.get_valid_actions()\n",
    "            action = agent.select_action(valid_actions, policy)\n",
    "            agent.perform_action(action)\n",
    "            reward = ACTION_REWARDS.get(action, 0)\n",
    "            agent.update_q_table(action, reward, agent.position)\n",
    "            if agent.is_terminal_state():\n",
    "                agent.reset()\n",
    "\n",
    "# Run Experiments\n",
    "run_experiment(agents, 'PRandom', STEPS_PRANDOM)  # Initial random phase\n",
    "run_experiment(agents, 'PExploit', STEPS_POLICY)  # Experiment 1c\n",
    "\n",
    "# Output results\n",
    "for agent in agents:\n",
    "    print(f\"{agent.name}'s Q-table after Experiment 1c:\")\n",
    "    print(\"State  |   N       E       S       W       P       D\")\n",
    "    print(\"---------------------------------------------------\")\n",
    "    for x in range(GRID_SIZE):\n",
    "        for y in range(GRID_SIZE):\n",
    "            q_values = agent.q_table[x, y]\n",
    "            print(f\"({x+1},{y+1})  |  \" + \"  \".join(f\"{q_value:6.2f}\" for q_value in q_values))\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e38cbbe8-4d09-4eeb-b7aa-a57cd6ab9740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "red's Q-table after Experiment 1c:\n",
      "State  |   N       E       S       W       P       D\n",
      "---------------------------------------------------\n",
      "(1,1)  |    4.63    0.00    0.00    5.10   16.62   17.34\n",
      "(1,2)  |    4.42    5.79    0.00    4.57   20.73   20.45\n",
      "(1,3)  |    8.29    6.33    0.00    7.26   14.09   21.63\n",
      "(1,4)  |    7.60    8.05    0.00    8.39   18.47   23.74\n",
      "(1,5)  |    2.62    4.30    0.00    0.00   19.21   12.37\n",
      "(2,1)  |   -0.83    0.00    2.62    0.00    8.19   10.38\n",
      "(2,2)  |   -0.45    0.62   -0.83    0.41    0.00    5.85\n",
      "(2,3)  |    3.02    3.25    2.55    1.64   13.90    0.00\n",
      "(2,4)  |    5.35    5.64    6.87    2.43   18.73    8.98\n",
      "(2,5)  |    0.33    1.57    0.48    0.00    7.17   14.13\n",
      "(3,1)  |   -0.83    0.00   -0.45   -0.91    0.00    0.00\n",
      "(3,2)  |    3.15    2.36    1.89    4.13   16.62    0.00\n",
      "(3,3)  |    6.24    3.31    5.34    2.92   15.68   15.09\n",
      "(3,4)  |    6.55    6.30    5.90    9.39    8.19   22.62\n",
      "(3,5)  |   10.16    3.56    9.55    0.00   26.00   23.97\n",
      "(4,1)  |    2.77    0.00    1.60    1.53    0.00   10.38\n",
      "(4,2)  |    3.02    6.46   -0.70    4.77   14.86   16.62\n",
      "(4,3)  |   10.29    9.65    6.46   10.45   24.05   24.29\n",
      "(4,4)  |    9.77    8.19    9.84    9.92   22.84   14.00\n",
      "(4,5)  |    6.02    7.36    5.91    0.00   10.38   21.20\n",
      "(5,1)  |    0.00    0.00    4.15    5.72    0.00   18.73\n",
      "(5,2)  |    0.00    2.29    4.34    2.23   13.90   16.39\n",
      "(5,3)  |    0.00    4.24    7.43    3.58   18.73   15.00\n",
      "(5,4)  |    0.00    6.68    3.74    2.92   16.07   19.33\n",
      "(5,5)  |    0.00    0.70   -0.45    0.00    7.17    5.85\n",
      "\n",
      "\n",
      "blue's Q-table after Experiment 1c:\n",
      "State  |   N       E       S       W       P       D\n",
      "---------------------------------------------------\n",
      "(1,1)  |    0.00    0.00    0.00    0.00    0.00    0.00\n",
      "(1,2)  |    2.12    0.00    0.00    3.14    5.85   11.40\n",
      "(1,3)  |    2.15    3.37    0.00    0.62   14.13   16.16\n",
      "(1,4)  |    0.48    0.62    0.00    1.21    0.00    5.85\n",
      "(1,5)  |    7.65    4.15    0.00    0.00   20.38   20.37\n",
      "(2,1)  |    4.95    0.00    0.00    3.98   18.73   13.48\n",
      "(2,2)  |   -0.45   -0.70   -0.70    1.16    7.17    5.85\n",
      "(2,3)  |    3.14   -0.45   -0.70    3.92    5.85   11.40\n",
      "(2,4)  |    6.08    6.88    7.23    7.31   23.78   22.56\n",
      "(2,5)  |    9.13    8.97    8.16    0.00   22.69   21.45\n",
      "(3,1)  |    6.25    0.00    6.10    8.01   23.72   22.61\n",
      "(3,2)  |    2.17    1.43    1.16    0.00    7.17    5.85\n",
      "(3,3)  |    0.41   -0.70   -0.45    0.62    0.00    5.85\n",
      "(3,4)  |    6.08    0.87    4.27    7.83   20.48   17.24\n",
      "(3,5)  |    6.03    6.43    8.63    0.00   19.80   20.43\n",
      "(4,1)  |    9.39    0.00    7.45    9.26   23.35   24.32\n",
      "(4,2)  |    9.56    9.57    8.77    8.41   23.97   21.40\n",
      "(4,3)  |    2.68    7.41    6.80    7.62   22.03   22.44\n",
      "(4,4)  |    4.58    7.08    9.44    7.47   21.96   23.16\n",
      "(4,5)  |    2.12    2.48    2.59    0.00   11.63   11.40\n",
      "(5,1)  |    0.00    0.00   10.36    8.69   24.98   24.69\n",
      "(5,2)  |    0.00   10.25    6.15    9.65   26.00   19.55\n",
      "(5,3)  |    0.00    6.05    4.70    4.42   17.23   12.37\n",
      "(5,4)  |    0.00    0.48    0.00    0.62    5.85    7.17\n",
      "(5,5)  |    0.00   -0.70   -0.45    0.00    7.17    5.85\n",
      "\n",
      "\n",
      "black's Q-table after Experiment 1c:\n",
      "State  |   N       E       S       W       P       D\n",
      "---------------------------------------------------\n",
      "(1,1)  |    0.87    0.00    0.00    0.48    5.85    0.00\n",
      "(1,2)  |    2.85    2.76    0.00    2.46    5.85   14.69\n",
      "(1,3)  |    1.59    1.16    0.00    1.59    7.17   10.68\n",
      "(1,4)  |    4.12    6.32    0.00    8.05   22.81   22.77\n",
      "(1,5)  |    8.21    8.25    0.00    0.00   23.97   10.06\n",
      "(2,1)  |    4.21    0.00    3.18    3.11   17.23   12.37\n",
      "(2,2)  |    0.87    1.80    0.91    3.63    5.85   11.40\n",
      "(2,3)  |    2.59    3.53    1.64    3.83   11.40   14.87\n",
      "(2,4)  |    2.89    5.09    1.60    1.21   17.26   16.16\n",
      "(2,5)  |    7.36    4.27    4.14    0.00    7.17   18.87\n",
      "(3,1)  |    6.98    0.00    4.27    4.89   16.25   22.62\n",
      "(3,2)  |    1.34    1.64    1.34    0.00   10.38    8.19\n",
      "(3,3)  |    2.15    1.89   -0.45    2.92    0.00   10.38\n",
      "(3,4)  |    2.79    3.49    2.62    3.73   10.38    8.19\n",
      "(3,5)  |    7.06    7.10    5.81    0.00   20.23   21.63\n",
      "(4,1)  |    5.75    0.00    6.13    5.50   20.37   14.53\n",
      "(4,2)  |    3.89    5.46    1.87    4.38   11.63   14.74\n",
      "(4,3)  |    1.64    3.28    3.02    3.38    5.85   26.00\n",
      "(4,4)  |   11.29    9.77    9.67   10.36   24.66   24.78\n",
      "(4,5)  |    7.12    5.79    4.23    0.00   19.41   19.07\n",
      "(5,1)  |    0.00    0.00    4.23    5.81   17.23    5.85\n",
      "(5,2)  |    0.00    8.64    4.68    7.46   23.20   23.34\n",
      "(5,3)  |    0.00    5.54    3.83    3.11   12.69   14.42\n",
      "(5,4)  |    0.00    9.91   10.60    8.31   16.28   25.05\n",
      "(5,5)  |    0.00    9.93   10.04    0.00   23.59   25.07\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Define the environment setup\n",
    "GRID_SIZE = 5\n",
    "PICKUP_LOCATIONS = {(0, 4): 5, (1, 3): 5, (4, 1): 5}\n",
    "DROPOFF_LOCATIONS = {(0, 0): 0, (2, 0): 0, (3, 4): 0}\n",
    "ACTIONS = ['N', 'E', 'S', 'W', 'P', 'D']\n",
    "ACTION_REWARDS = {'P': 13, 'D': 13, 'N': -1, 'E': -1, 'S': -1, 'W': -1}\n",
    "\n",
    "# Hyperparameters\n",
    "LEARNING_RATE = 0.45\n",
    "DISCOUNT_FACTOR = 0.5\n",
    "STEPS_PRANDOM = 500  # Initial random steps\n",
    "STEPS_POLICY = 8500  # Steps for PExploit policy\n",
    "\n",
    "# Seed for reproducibility\n",
    "random.seed(4)\n",
    "np.random.seed(4)\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, start_position, name):\n",
    "        self.position = start_position\n",
    "        self.name = name\n",
    "        self.carrying = False\n",
    "        self.q_table = np.zeros((GRID_SIZE, GRID_SIZE, len(ACTIONS)))\n",
    "\n",
    "    def get_valid_actions(self):\n",
    "        x, y = self.position\n",
    "        valid_actions = ACTIONS.copy()\n",
    "        if x == 0: valid_actions.remove('N')\n",
    "        if x == GRID_SIZE - 1: valid_actions.remove('S')\n",
    "        if y == 0: valid_actions.remove('W')\n",
    "        if y == GRID_SIZE - 1: valid_actions.remove('E')\n",
    "        if self.carrying and (x, y) in DROPOFF_LOCATIONS:\n",
    "            valid_actions.append('D')\n",
    "        elif not self.carrying and (x, y) in PICKUP_LOCATIONS and PICKUP_LOCATIONS[(x, y)] > 0:\n",
    "            valid_actions.append('P')\n",
    "        return valid_actions\n",
    "\n",
    "    def select_action(self, valid_actions, policy):\n",
    "        if not valid_actions:\n",
    "            return None\n",
    "        if policy == 'PRandom':\n",
    "            return random.choice(valid_actions)\n",
    "        elif policy == 'PExploit':\n",
    "            q_values = [self.q_table[x, y, ACTIONS.index(a)] for a in valid_actions]\n",
    "            max_value = max(q_values)\n",
    "            best_actions = [a for a, q in zip(valid_actions, q_values) if q == max_value]\n",
    "            return random.choice(best_actions)\n",
    "\n",
    "    def perform_action(self, action):\n",
    "        if action is None:\n",
    "            return\n",
    "        move_delta = {'N': (-1, 0), 'S': (1, 0), 'E': (0, 1), 'W': (0, -1)}\n",
    "        x, y = self.position\n",
    "        x += move_delta.get(action, (0, 0))[0]\n",
    "        y += move_delta.get(action, (0, 0))[1]\n",
    "        if action == 'P' and (x, y) in PICKUP_LOCATIONS:\n",
    "            self.carrying = True\n",
    "            PICKUP_LOCATIONS[(x, y)] -= 1\n",
    "        elif action == 'D' and (x, y) in DROPOFF_LOCATIONS:\n",
    "            self.carrying = False\n",
    "            DROPOFF_LOCATIONS[(x, y)] += 1\n",
    "        self.position = (x, y)\n",
    "\n",
    "    def update_q_table(self, action, reward, next_position):\n",
    "        if action is None:\n",
    "            return\n",
    "        old_q = self.q_table[self.position[0], self.position[1], ACTIONS.index(action)]\n",
    "        next_max_q = np.max(self.q_table[next_position[0], next_position[1]])\n",
    "        self.q_table[self.position[0], self.position[1], ACTIONS.index(action)] = \\\n",
    "            (1 - LEARNING_RATE) * old_q + LEARNING_RATE * (reward + DISCOUNT_FACTOR * next_max_q)\n",
    "\n",
    "    def is_terminal_state(self):\n",
    "        return all(value == 5 for value in DROPOFF_LOCATIONS.values())\n",
    "\n",
    "    def reset(self):\n",
    "        self.position = initial_positions[self.name]\n",
    "        self.carrying = False\n",
    "        for loc in PICKUP_LOCATIONS:\n",
    "            PICKUP_LOCATIONS[loc] = 5\n",
    "        for loc in DROPOFF_LOCATIONS:\n",
    "            DROPOFF_LOCATIONS[loc] = 0\n",
    "\n",
    "initial_positions = {'red': (2, 2), 'blue': (4, 2), 'black': (0, 2)}\n",
    "agents = [Agent(initial_positions[name], name) for name in initial_positions]\n",
    "\n",
    "def run_experiment(agents, policy, steps):\n",
    "    for step in range(steps):\n",
    "        for agent in agents:\n",
    "            valid_actions = agent.get_valid_actions()\n",
    "            action = agent.select_action(valid_actions, policy)\n",
    "            agent.perform_action(action)\n",
    "            reward = ACTION_REWARDS.get(action, 0)\n",
    "            agent.update_q_table(action, reward, agent.position)\n",
    "            if agent.is_terminal_state():\n",
    "                agent.reset()\n",
    "\n",
    "# Run Experiments\n",
    "run_experiment(agents, 'PRandom', STEPS_PRANDOM)  # Initial random phase\n",
    "run_experiment(agents, 'PExploit', STEPS_POLICY)  # Experiment 1c\n",
    "\n",
    "# Output results\n",
    "for agent in agents:\n",
    "    print(f\"{agent.name}'s Q-table after Experiment 1c:\")\n",
    "    print(\"State  |   N       E       S       W       P       D\")\n",
    "    print(\"---------------------------------------------------\")\n",
    "    for x in range(GRID_SIZE):\n",
    "        for y in range(GRID_SIZE):\n",
    "            q_values = agent.q_table[x, y]\n",
    "            print(f\"({x+1},{y+1})  |  \" + \"  \".join(f\"{q_value:6.2f}\" for q_value in q_values))\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fba0e085-dbb6-40db-a091-dd3b1f550275",
   "metadata": {},
   "source": [
    "(Discussion) Analyzing the effects of using the 3 different learning rates on the system performance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ef1b99-344a-4fdb-8f98-6a193066689d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "74f0afcd-2a9d-43f6-9d7a-94981a9ef51d",
   "metadata": {},
   "source": [
    "<h3> EXP4 From 1c alpha =0.3, Gamma=0.5  ,Q-learning three pickup locations to: (4,2), (3,3) and (2,4)</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a37b308-789f-48c2-a4d2-6f19b33a6517",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Define the environment setup\n",
    "GRID_SIZE = 5\n",
    "PICKUP_LOCATIONS = {(0, 4): 5, (1, 3): 5, (4, 1): 5}\n",
    "DROPOFF_LOCATIONS = {(0, 0): 0, (2, 0): 0, (3, 4): 0}\n",
    "ACTIONS = ['N', 'E', 'S', 'W', 'P', 'D']\n",
    "ACTION_REWARDS = {'P': 13, 'D': 13, 'N': -1, 'E': -1, 'S': -1, 'W': -1}\n",
    "\n",
    "# Hyperparameters\n",
    "LEARNING_RATE = 0.45\n",
    "DISCOUNT_FACTOR = 0.5\n",
    "STEPS_PRANDOM = 500  # Initial random steps\n",
    "STEPS_POLICY = 8500  # Steps for PExploit policy\n",
    "\n",
    "# Seed for reproducibility\n",
    "random.seed(4)\n",
    "np.random.seed(4)\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, start_position, name):\n",
    "        self.position = start_position\n",
    "        self.name = name\n",
    "        self.carrying = False\n",
    "        self.q_table = np.zeros((GRID_SIZE, GRID_SIZE, len(ACTIONS)))\n",
    "\n",
    "    def get_valid_actions(self):\n",
    "        x, y = self.position\n",
    "        valid_actions = ACTIONS.copy()\n",
    "        if x == 0: valid_actions.remove('N')\n",
    "        if x == GRID_SIZE - 1: valid_actions.remove('S')\n",
    "        if y == 0: valid_actions.remove('W')\n",
    "        if y == GRID_SIZE - 1: valid_actions.remove('E')\n",
    "        if self.carrying and (x, y) in DROPOFF_LOCATIONS:\n",
    "            valid_actions.append('D')\n",
    "        elif not self.carrying and (x, y) in PICKUP_LOCATIONS and PICKUP_LOCATIONS[(x, y)] > 0:\n",
    "            valid_actions.append('P')\n",
    "        return valid_actions\n",
    "\n",
    "    def select_action(self, valid_actions, policy):\n",
    "        if not valid_actions:\n",
    "            return None\n",
    "        if policy == 'PRandom':\n",
    "            return random.choice(valid_actions)\n",
    "        elif policy == 'PExploit':\n",
    "            q_values = [self.q_table[x, y, ACTIONS.index(a)] for a in valid_actions]\n",
    "            max_value = max(q_values)\n",
    "            best_actions = [a for a, q in zip(valid_actions, q_values) if q == max_value]\n",
    "            return random.choice(best_actions)\n",
    "\n",
    "    def perform_action(self, action):\n",
    "        if action is None:\n",
    "            return\n",
    "        move_delta = {'N': (-1, 0), 'S': (1, 0), 'E': (0, 1), 'W': (0, -1)}\n",
    "        x, y = self.position\n",
    "        x += move_delta.get(action, (0, 0))[0]\n",
    "        y += move_delta.get(action, (0, 0))[1]\n",
    "        if action == 'P' and (x, y) in PICKUP_LOCATIONS:\n",
    "            self.carrying = True\n",
    "            PICKUP_LOCATIONS[(x, y)] -= 1\n",
    "        elif action == 'D' and (x, y) in DROPOFF_LOCATIONS:\n",
    "            self.carrying = False\n",
    "            DROPOFF_LOCATIONS[(x, y)] += 1\n",
    "        self.position = (x, y)\n",
    "\n",
    "    def update_q_table(self, action, reward, next_position):\n",
    "        if action is None:\n",
    "            return\n",
    "        old_q = self.q_table[self.position[0], self.position[1], ACTIONS.index(action)]\n",
    "        next_max_q = np.max(self.q_table[next_position[0], next_position[1]])\n",
    "        self.q_table[self.position[0], self.position[1], ACTIONS.index(action)] = \\\n",
    "            (1 - LEARNING_RATE) * old_q + LEARNING_RATE * (reward + DISCOUNT_FACTOR * next_max_q)\n",
    "\n",
    "    def is_terminal_state(self):\n",
    "        return all(value == 5 for value in DROPOFF_LOCATIONS.values())\n",
    "\n",
    "    def reset(self):\n",
    "        self.position = initial_positions[self.name]\n",
    "        self.carrying = False\n",
    "        for loc in PICKUP_LOCATIONS:\n",
    "            PICKUP_LOCATIONS[loc] = 5\n",
    "        for loc in DROPOFF_LOCATIONS:\n",
    "            DROPOFF_LOCATIONS[loc] = 0\n",
    "\n",
    "initial_positions = {'red': (2, 2), 'blue': (4, 2), 'black': (0, 2)}\n",
    "agents = [Agent(initial_positions[name], name) for name in initial_positions]\n",
    "\n",
    "def run_experiment(agents, policy, steps):\n",
    "    for step in range(steps):\n",
    "        for agent in agents:\n",
    "            valid_actions = agent.get_valid_actions()\n",
    "            action = agent.select_action(valid_actions, policy)\n",
    "            agent.perform_action(action)\n",
    "            reward = ACTION_REWARDS.get(action, 0)\n",
    "            agent.update_q_table(action, reward, agent.position)\n",
    "            if agent.is_terminal_state():\n",
    "                agent.reset()\n",
    "\n",
    "# Run Experiments\n",
    "run_experiment(agents, 'PRandom', STEPS_PRANDOM)  # Initial random phase\n",
    "run_experiment(agents, 'PExploit', STEPS_POLICY)  # Experiment 1c\n",
    "\n",
    "# Output results\n",
    "for agent in agents:\n",
    "    print(f\"{agent.name}'s Q-table after Experiment 1c:\")\n",
    "    print(\"State  |   N       E       S       W       P       D\")\n",
    "    print(\"---------------------------------------------------\")\n",
    "    for x in range(GRID_SIZE):\n",
    "        for y in range(GRID_SIZE):\n",
    "            q_values = agent.q_table[x, y]\n",
    "            print(f\"({x+1},{y+1})  |  \" + \"  \".join(f\"{q_value:6.2f}\" for q_value in q_values))\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905b0a35-9fa2-47bc-a24d-9338a64411cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e41a429-e595-4791-b5a0-177cef3de25d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35cd3a81-926f-4866-a22b-2e2bebd8a362",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffa4c41-814a-4522-8779-ffa732d17028",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "17ee9218-bb7e-49dd-a5c1-a7dc6a5adb32",
   "metadata": {},
   "source": [
    " Discussion))) When interpreting the results of this experiment center on analyzing on how well the learning strategy was able to adapt to the change of the pickup locations and to which extend it was able to learn “new” paths and unlearn “old” paths which became obsolete"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dcd45eb-fce7-481e-8b71-f59e5187c4eb",
   "metadata": {},
   "source": [
    "note:For all EXPERMINENTS, if a terminal state is reached, restart the experiment by resetting the PD world to the initial state, but do not reset the Q-table. Run each experiment twice, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6164df0f-fd7b-412a-804f-6cde8fc9523d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4bb433e-a149-496c-b356-0c0f29df1a61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e80fca5-d8d4-40e3-aa18-76333188b806",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95c0eb6-90eb-44cd-87eb-7e4600b9c715",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a3d98b-a538-4d8a-80a8-16112319648a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ae5bf02e-a3a3-45b5-8a38-bbea12c77e8c",
   "metadata": {},
   "source": [
    "Report  and interpret the results; e.g., utilities computed, rewards obtained in various stages of each experiment. \n",
    "\n",
    "Assess which experiment obtained the best results . Next, analyze the various q-tables you created and try identify attractive paths  in the obtained q-tables, if there are any. Moreover, briefly assess if your system gets better after it solved a few PD-world problems—reached the terminal state at least once. Briefly analyze to which extend the results of the two different runs agree and disagree in the 4 experiments. Analyze agent coordination for experiments 1.c and 4. Finally, analyze how well the approach adapted to change in the fourth experiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89db4998-c64f-427c-acb2-c346d95963a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
